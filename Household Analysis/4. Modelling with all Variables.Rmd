---
title: "Predicting Ignorance New"
author: "Ahmad Risal"
date: "2025-06-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 0: Initial Data Preparation

```{r}
df_model_raw <- readRDS("final_analysis_data.rds")
```

```{r}
# Identify variables with more than one class
multi_class_vars <- sapply(df_model_raw, function(x) length(class(x)) > 1)

# Show variable names and their class attributes
multi_class_details <- sapply(df_model_raw[multi_class_vars], class)

# Display the result
multi_class_details
```

```{r}
# Identify variables with both "ordered" and "factor"
multi_class_vars <- sapply(df_model_raw, function(x) all(c("ordered", "factor") %in% class(x)))

# Forcefully convert all ordered factors to plain factors
df_model_raw[multi_class_vars] <- lapply(
  df_model_raw[multi_class_vars],
  function(x) factor(as.character(x))
)
```

```{r}
# Check updated classes
sapply(df_model_raw[multi_class_vars], class)
```

```{r}
# Identify character columns
char_vars <- sapply(df_model_raw, is.character)

# Convert them to factors
df_model_raw[char_vars] <- lapply(df_model_raw[char_vars], as.factor)
```

```{r}
table(sapply(df_model_raw, class))
```

```{r}
# Load required libraries
library(dplyr)
library(caret)

# 2. Standardise column names
names(df_model_raw) <- make.names(names(df_model_raw), unique = TRUE)
cat("Column names standardised.\n")

# 3. Clean factor levels by replacing spaces with dots
df_model_raw <- df_model_raw %>%
  mutate(across(where(is.factor), ~ {
    levels(.) <- make.names(levels(.), unique = FALSE)
    .
  }))
cat("Factor levels cleaned using make.names().\n")

# 4. Ensure MISKIN_KAKO is a factor with correct labels
# Level order: 0 = TIDAK.MISKIN (reference), 1 = MISKIN (positive)
df_model_raw <- df_model_raw %>%
  mutate(MISKIN_KAKO = factor(MISKIN_KAKO, levels = c(0, 1), labels = c("TIDAK.MISKIN", "MISKIN")))
cat("MISKIN_KAKO formatted as factor with labels.\n")

# 5. Check for missing values
na_summary <- sapply(df_model_raw, function(x) sum(is.na(x)))
na_summary_display <- na_summary[na_summary > 0]
if(length(na_summary_display) > 0) {
  cat("\nVariables with missing values:\n")
  print(na_summary_display)
} else {
  cat("\nNo missing values found.\n")
}

# 6. Create dummy variables
categorical_features <- names(df_model_raw)[sapply(df_model_raw, is.factor)]
categorical_features <- setdiff(categorical_features, "MISKIN_KAKO") # Exclude target

if (length(categorical_features) > 0) {
  # Create dummy variables, dropping one level per factor to avoid multicollinearity
  # --- THIS IS THE MODIFIED LINE ---
  dummy_model <- dummyVars(~ ., data = df_model_raw[, categorical_features], fullRank = TRUE)
  # ---------------------------------
  
  encoded_features <- predict(dummy_model, newdata = df_model_raw[, categorical_features])
  
  # Convert to data.frame to ensure compatibility
  encoded_features <- as.data.frame(encoded_features)

  # Bind encoded features with remaining (non-categorical + target)
  data_ml <- df_model_raw %>%
    select(-one_of(categorical_features)) %>% # remove original factor columns
    bind_cols(encoded_features)
} else {
  data_ml <- df_model_raw
}

# 7. Final structure and summary
cat("\nStructure of final modelling dataset (data_ml):\n")
print(str(data_ml))
cat("\nPreview column names:\n")
print(head(names(data_ml)))
cat("\nClass balance:\n")
print(table(data_ml$MISKIN_KAKO))

```

# Section 1: Universal Train and Test Split

```{r}
library(caret)
library(dplyr)

# 1. Convert MISKIN_KAKO to numeric (0 = TIDAK.MISKIN, 1 = MISKIN)
data_ml <- data_ml %>%
  mutate(MISKIN_KAKO = as.integer(MISKIN_KAKO == "MISKIN"))
cat("MISKIN_KAKO converted to numeric (0 = TIDAK.MISKIN, 1 = MISKIN).\n")

# 2. Set random seed for reproducibility
set.seed(123)

# 3. Stratified sampling (75% train, 25% test)
train_index_universal <- createDataPartition(
  y = data_ml$MISKIN_KAKO,
  p = 0.75,
  list = FALSE
)

# 4. Split into training and testing datasets
train_data_universal <- data_ml[train_index_universal, ]
test_data_universal  <- data_ml[-train_index_universal, ]

# 5. Summary of split
cat("\nDimensions of train_data_universal:\n")
print(dim(train_data_universal))
cat("Class balance in train_data_universal:\n")
print(prop.table(table(train_data_universal$MISKIN_KAKO)))

cat("\nDimensions of test_data_universal:\n")
print(dim(test_data_universal))
cat("Class balance in test_data_universal:\n")
print(prop.table(table(test_data_universal$MISKIN_KAKO)))
```

# **Section 2: Apply SMOTE on Universal Training Data**

```{r}
# library(smotefamily)
# library(dplyr)
# library(ggplot2)
# 
# cat("Step 2: Applying SMOTE on the universal training data...\n")
# 
# # --- 2.1 Prepare data for SMOTE ---
# # Separate features and target
# predictor_cols_for_smote <- setdiff(names(train_data_universal), "MISKIN_KAKO")
# X_train_universal <- train_data_universal[, predictor_cols_for_smote, drop = FALSE]
# y_train_universal <- train_data_universal$MISKIN_KAKO  # numeric (0 = TIDAK.MISKIN, 1 = MISKIN)
# 
# # Show class distribution BEFORE SMOTE
# cat("\nClass distribution BEFORE SMOTE:\n")
# initial_table <- table(y_train_universal)
# print(initial_table)
# 
# # Visualise before SMOTE
# df_before <- data.frame(
#   Class = factor(ifelse(y_train_universal == 1, "MISKIN", "TIDAK.MISKIN"), levels = c("TIDAK.MISKIN", "MISKIN"))
# )
# 
# ggplot(df_before, aes(x = Class, fill = Class)) +
#   geom_bar() +
#   labs(title = "Class Distribution BEFORE SMOTE", y = "Count") +
#   theme_minimal()
# 
# # --- 2.2 Calculate dup_size for ~50:50 balance ---
# minority <- min(initial_table)
# majority <- max(initial_table)
# dup_size <- floor((majority / minority) - 1)
# dup_size <- max(dup_size, 1)  # Ensure at least 1
# 
# # --- 2.3 Apply SMOTE ---
# cat(paste0("Applying SMOTE with K = 5 and dup_size = ", dup_size, "...\n"))
# smote_result <- SMOTE(X_train_universal, y_train_universal, K = 5, dup_size = dup_size)
# 
# train_smote_universal <- smote_result$data
# names(train_smote_universal)[ncol(train_smote_universal)] <- "MISKIN_KAKO"  # Rename last column
# 
# # Ensure target is numeric 0/1
# train_smote_universal$MISKIN_KAKO <- as.integer(train_smote_universal$MISKIN_KAKO)
# 
# # --- 2.4 Visualise BEFORE and AFTER in the same chart ---
# 
# # Combine data into one long-format dataframe for plotting
# df_before <- data.frame(
#   Class = factor(ifelse(y_train_universal == 1, "MISKIN", "TIDAK.MISKIN"), levels = c("TIDAK.MISKIN", "MISKIN")),
#   Source = "Before SMOTE"
# )
# 
# df_after <- data.frame(
#   Class = factor(ifelse(train_smote_universal$MISKIN_KAKO == 1, "MISKIN", "TIDAK.MISKIN"), levels = c("TIDAK.MISKIN", "MISKIN")),
#   Source = "After SMOTE"
# )
# 
# df_combined <- bind_rows(df_before, df_after)
# 
# # Summarise counts for label placement
# label_df <- df_combined %>%
#   group_by(Source, Class) %>%
#   summarise(Count = n(), .groups = "drop")
# 
# # Plot
# ggplot(label_df, aes(x = Class, y = Count, fill = Source)) +
#   geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
#   geom_text(aes(label = Count), 
#             position = position_dodge(width = 0.8), 
#             vjust = -0.2, size = 4) +
#   labs(title = "Class Distribution Before and After SMOTE",
#        x = "Class",
#        y = "Count") +
#   scale_fill_manual(values = c("Before SMOTE" = "#1f77b4", "After SMOTE" = "#ff7f0e")) +
#   theme_minimal(base_size = 12)
# 
# # --- 2.5 Check for missing values ---
# cat("\nChecking for NA values in SMOTE dataset...\n")
# na_summary_smote <- sapply(train_smote_universal, function(x) sum(is.na(x)))
# print(na_summary_smote[na_summary_smote > 0])
```

```{r}
# Load required libraries
library(dplyr)
library(themis)

cat("Step 2: Applying SMOTE on the universal training data for a 60:40 balance...\n")

# --- 2.1 Show class distribution BEFORE SMOTE ---
cat("\nClass distribution BEFORE SMOTE:\n")
print(table(train_data_universal$MISKIN_KAKO))
cat("\nProportions BEFORE SMOTE:\n")
print(prop.table(table(train_data_universal$MISKIN_KAKO)))

# --- 2.2 Apply SMOTE using themis ---
# Themis needs the target variable to be a factor
train_data_for_smote <- train_data_universal %>%
  mutate(MISKIN_KAKO = factor(MISKIN_KAKO, levels = c(0, 1), labels = c("TIDAK.MISKIN", "MISKIN")))

# Target ratio: 40% minority (MISKIN) and 60% majority (TIDAK.MISKIN)
target_ratio <- 40 / 60 

# Apply SMOTE, explicitly calling the function from the themis package
set.seed(123)
train_smote_universal <- themis::smote(
  train_data_for_smote, 
  var = "MISKIN_KAKO",
  over_ratio = target_ratio
)

# Convert target variable back to numeric (0/1) for modelling
train_smote_universal <- train_smote_universal %>%
  mutate(MISKIN_KAKO = as.integer(MISKIN_KAKO == "MISKIN"))
  
# --- 2.3 Show class distribution AFTER SMOTE ---
cat("\nClass distribution AFTER SMOTE:\n")
print(table(train_smote_universal$MISKIN_KAKO))
cat("\nProportions AFTER SMOTE:\n")
print(prop.table(table(train_smote_universal$MISKIN_KAKO)))

cat("\nSMOTE application complete. The dataset 'train_smote_universal' is ready.\n")
```

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# 1. Prepare data for plotting
# Create a dataframe for the original data
df_before <- data.frame(
  MISKIN_KAKO = train_data_universal$MISKIN_KAKO, # Data before SMOTE
  Source = "Before SMOTE"
)

# Create a dataframe for the balanced data
df_after <- data.frame(
  MISKIN_KAKO = train_smote_universal$MISKIN_KAKO, # Data after SMOTE
  Source = "After SMOTE"
)

# 2. Combine the two dataframes
df_combined <- bind_rows(df_before, df_after)

# 3. Convert the target variable to a descriptive factor for better labels
df_combined <- df_combined %>%
  mutate(MISKIN_KAKO_label = factor(MISKIN_KAKO,
                             levels = c(0, 1),
                             labels = c("Not Poor", "Poor")))

# 4. Create the bar chart
ggplot(df_combined, aes(x = MISKIN_KAKO_label, fill = Source)) +
  geom_bar(position = "dodge") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)), # Automatically adds the count as a label
    position = position_dodge(width = 0.9),
    vjust = -0.5, # Adjust vertical position to be above the bar
    size = 4
  ) +
  scale_fill_manual(values = c("Before SMOTE" = "#1f77b4", "After SMOTE" = "#ff7f0e")) +
  labs(
    title = "Class Distribution Before and After SMOTE",
    x = "Poverty Status (MISKIN_KAKO)",
    y = "Number of Samples",
    fill = "Dataset"
  ) +
  theme_minimal(base_size = 14)
```

# Section 4: Modelling

## 4.1: Reusable Functions

```{r}
# ===================================================================
# Final Reusable Functions for Modelling & Evaluation
# ===================================================================

# 1. Load All Required Libraries
# -------------------------------------------------------------------
library(caret)
library(pROC)
library(dplyr)
library(tidyr)
library(ggplot2)
library(purrr)
library(tibble)
library(readr)
library(writexl)
library(doParallel)


# 2. Setup Parallel Processing & Global Result Lists
# -------------------------------------------------------------------
# Use n-1 cores to leave one free for system processes
num_cores <- detectCores() - 1
if (num_cores < 1) num_cores <- 1
registerDoParallel(cores = num_cores)
cat(paste0("Registered ", num_cores, " cores for parallel processing.\n"))

# Initialise global lists to store results from all models
all_model_performance_metrics <- list()
all_cv_accuracy_results <- list()
all_roc_data <- list()
all_model_parameters <- list()

cat("Global result lists created.\n")


# 3. Reusable Function Definitions
# -------------------------------------------------------------------

# --- Function 1: Train and Evaluate a Model ---
train_and_evaluate_model <- function(
  train_data,
  test_data,
  target_variable_name,
  model_name,
  method_caret,
  model_formula   = NULL,
  tuneGrid        = NULL,
  preProc_methods = NULL,
  ...
) {
  cat(paste0("\n--- Training and Evaluating ", model_name, " Model ---\n"))

  # Control for 5-fold CV optimising ROC
  fitControl <- trainControl(
    method            = "cv",
    number            = 5,
    savePredictions   = "final",
    classProbs        = TRUE,
    summaryFunction   = twoClassSummary,
    allowParallel     = TRUE
  )

  # Convert numeric binary target to factor for classification
  train_data[[target_variable_name]] <- factor(
    ifelse(train_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
    levels = c("NotPoor", "Poor")
  )
  test_data[[target_variable_name]] <- factor(
    ifelse(test_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
    levels = c("NotPoor", "Poor")
  )

  # Build formula
  formula_obj <- if (is.null(model_formula)) 
    as.formula(paste0(target_variable_name, " ~ .")) 
  else model_formula

  # Set seed and train
  set.seed(456)
  trained_model <- train(
    formula_obj,
    data      = train_data,
    method    = method_caret,
    trControl = fitControl,
    tuneGrid  = tuneGrid,
    preProcess= preProc_methods,
    metric    = "ROC",
    ...
  )
  cat(paste0(model_name, " model training completed.\n"))

  # Extract CV accuracy for diagnostic plot
  preds <- trained_model$pred
  if (!is.null(trained_model$bestTune)) {
    for (param in names(trained_model$bestTune)) {
      preds <- preds[preds[[param]] == trained_model$bestTune[[param]], ]
    }
  }
  cv_results_df <- preds %>%
    group_by(Resample) %>%
    summarize(Accuracy = mean(pred == obs, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(Model = model_name)
  all_cv_accuracy_results[[model_name]] <<- cv_results_df

  # Test set evaluation with optimal threshold for F1-Score
  test_probs <- predict(trained_model, test_data, type = "prob")[, "Poor"]
  actuals <- test_data[[target_variable_name]]
  roc_obj <- roc(response = actuals, predictor = test_probs, levels = c("NotPoor", "Poor"))
                   
  # Find optimal threshold that maximizes F1-Score
  coords_df <- coords(roc_obj, "all", ret = c("threshold", "precision", "recall")) %>%
    as.data.frame() %>%
    mutate(f1 = 2 * (precision * recall) / (precision + recall)) %>%
    filter(is.finite(f1))

  optimal_threshold <- coords_df %>%
    slice_max(order_by = f1, n = 1) %>%
    pull(threshold)

  cat(paste0("Optimal threshold found to maximise F1-Score: ", round(optimal_threshold, 4), "\n"))

  # Apply the optimal threshold for predictions
  test_preds <- factor(ifelse(test_probs >= optimal_threshold, "Poor", "NotPoor"),
                       levels = c("NotPoor", "Poor"))
  
  cm <- confusionMatrix(test_preds, actuals, positive = "Poor")

  # Store ROC curve data
  all_roc_data[[model_name]] <<- data.frame(
    FPR   = 1 - roc_obj$specificities,
    TPR   = roc_obj$sensitivities,
    Model = model_name
  )

  # Store final performance metrics
  all_model_performance_metrics[[model_name]] <<- list(
    Accuracy        = cm$overall["Accuracy"],
    Sensitivity     = cm$byClass["Sensitivity"], # Recall
    Specificity     = cm$byClass["Specificity"],
    Precision       = cm$byClass["Precision"],
    F1_Score        = cm$byClass["F1"],
    AUC             = as.numeric(auc(roc_obj)),
    Best_Threshold  = optimal_threshold
  )

  # Store tuning parameters
  if (!is.null(trained_model$bestTune)) {
    param_str <- paste(names(trained_model$bestTune), trained_model$bestTune, sep = "=", collapse = ", ")
  } else {
    param_str <- "—"
  }
  all_model_parameters[[model_name]] <<- param_str

  cat(paste0("Evaluation for ", model_name, " on test data completed.\n"))
  return(trained_model)
}


# --- Function 2: Create a Summary Comparison Table ---
create_comparison_table <- function() {
  if (length(all_model_performance_metrics) == 0) {
    cat("No model performance metrics available.\n")
    return(NULL)
  }
  
  # Safely bind model names and metrics into a data frame
  df_summary <- lapply(names(all_model_performance_metrics), function(model_name) {
    metrics <- all_model_performance_metrics[[model_name]]
    params  <- all_model_parameters[[model_name]]
    
    data.frame(
      Model           = model_name,
      Accuracy        = round(metrics$Accuracy, 4),
      F1_Score        = round(metrics$F1_Score, 4),
      AUC             = round(metrics$AUC, 4),
      Sensitivity     = round(metrics$Sensitivity, 4),
      Specificity     = round(metrics$Specificity, 4),
      Precision       = round(metrics$Precision, 4),
      Best_Threshold  = round(metrics$Best_Threshold, 4),
      Parameters      = paste(params, collapse = ", ")
    )
  }) %>%
    bind_rows()

  return(df_summary)
}


# --- Function 3: Plot Cross-Validation Accuracy Boxplot (Diagnostic) ---
plot_cv_accuracy_boxplot <- function() {
  if (length(all_cv_accuracy_results) == 0) return()
  df <- bind_rows(all_cv_accuracy_results) %>% filter(is.finite(Accuracy))
  p <- ggplot(df, aes(x = reorder(Model, Accuracy, median), y = Accuracy, fill = Model)) +
    geom_boxplot() +
    labs(
      title = "Model Stability: Cross-Validation Accuracy",
      subtitle = "Shows performance distribution during training on SMOTE data",
      x = "Model", y = "Accuracy"
    ) +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = 25, hjust = 1))
  print(p)
}


# --- Function 4: Plot Final Performance Bar Chart ---
plot_all_performance_bar_chart <- function(df, metric_to_plot = "F1_Score") {
  if (!metric_to_plot %in% names(df)) stop("Metric not found: ", metric_to_plot)
  
  p <- ggplot(df, aes_string(x = sprintf("reorder(Model, -%s)", metric_to_plot), y = metric_to_plot, fill = "Model")) +
    geom_col(width = 0.7) +
    geom_text(aes_string(label = metric_to_plot), vjust = -0.5, size = 3.5) +
    labs(
      title = paste("Final Model Performance on Test Set"),
      subtitle = paste("Metric:", metric_to_plot),
      x = "Model", y = metric_to_plot
    ) +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = 25, hjust = 1))
  print(p)
}


# --- Function 5: Plot All ROC Curves ---
plot_all_roc_curves <- function() {
  if (length(all_roc_data) == 0) return()
  df <- bind_rows(all_roc_data)
  
  # Add AUC values for the legend
  auc_labels <- create_comparison_table() %>%
    select(Model, AUC) %>%
    mutate(label = paste0(Model, " (AUC = ", AUC, ")"))
    
  df <- df %>% left_join(auc_labels, by = "Model")

  p <- ggplot(df, aes(x = FPR, y = TPR, colour = label)) +
    geom_line(linewidth = 1.1) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "grey") +
    labs(
      title = "ROC Curves for All Models on Test Set",
      x = "False Positive Rate (1 - Specificity)", 
      y = "True Positive Rate (Sensitivity)",
      colour = "Model"
    ) +
    theme_minimal()
  print(p)
}


# --- Function 6: Export Summary Table to Excel ---
export_summary_to_excel <- function(summary_df, file_path = "model_performance_summary.xlsx") {
  if (is.null(summary_df) || nrow(summary_df) == 0) {
    cat("Summary data frame is empty. Nothing to export.\n")
    return()
  }
  
  write_xlsx(summary_df, path = file_path)
  cat(paste0("\nFinal model performance summary exported to: ", file_path, "\n"))
}

```

```{r}
# Global lists to store results
all_model_performance_metrics <- list()
all_cv_accuracy_results <- list()
all_roc_data <- list()
all_model_parameters <- list()

cat("Global result lists created.\n")
```

### 4.1.2 Reusable function maximise sensitivity

```{r}
# # -------------------------------------------------------------------
# # Initialise global lists for sensitivity-focused evaluation
# # -------------------------------------------------------------------
# all_model_performance_metrics_sens <- list()
# all_cv_accuracy_results_sens     <- list()
# all_roc_data_sens                <- list()
# all_model_parameters_sens        <- list()
# 
# cat("Global result lists for sensitivity-focus created.\n")
# 
# 
# # -------------------------------------------------------------------
# # Function: Train & evaluate model, picking threshold to maximise Sensitivity
# # -------------------------------------------------------------------
# train_and_evaluate_model_sens <- function(
#   train_data,
#   test_data,
#   target_variable_name,
#   model_name,
#   method_caret,
#   model_formula   = NULL,
#   tuneGrid        = NULL,
#   preProc_methods = NULL,
#   min_specificity = 0.4,
#   ...
# ) {
#   cat(paste0("\n--- Training and Evaluating ", model_name,
#              " (sensitivity-focus) Model ---\n"))
# 
#   # same CV control as before
#   fitControl <- trainControl(
#     method            = "cv",
#     number            = 5,
#     savePredictions   = "final",
#     classProbs        = TRUE,
#     summaryFunction   = twoClassSummary,
#     allowParallel     = TRUE
#   )
# 
#   # factorise target
#   train_data[[target_variable_name]] <- factor(
#     ifelse(train_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
#     levels = c("NotPoor", "Poor")
#   )
#   test_data[[target_variable_name]] <- factor(
#     ifelse(test_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
#     levels = c("NotPoor", "Poor")
#   )
# 
#   # build formula
#   formula_obj <- if (is.null(model_formula)) 
#     as.formula(paste0(target_variable_name, " ~ .")) 
#   else model_formula
# 
#   set.seed(456)
#   trained_model <- train(
#     formula_obj,
#     data      = train_data,
#     method    = method_caret,
#     trControl = fitControl,
#     tuneGrid  = tuneGrid,
#     preProcess= preProc_methods,
#     metric    = "ROC",
#     ...
#   )
#   cat(paste0(model_name, " training completed.\n"))
# 
#   # store CV accuracy
#   preds <- trained_model$pred
#   if (!is.null(trained_model$bestTune)) {
#     for (param in names(trained_model$bestTune)) {
#       preds <- preds[preds[[param]] == trained_model$bestTune[[param]], ]
#     }
#   }
#   cv_df <- preds %>%
#     group_by(Resample) %>%
#     summarize(Accuracy = mean(pred == obs, na.rm = TRUE)) %>%
#     ungroup() %>%
#     mutate(Model = model_name)
#   all_cv_accuracy_results_sens[[model_name]] <<- cv_df
# 
#   # ROC & threshold search
#   test_probs <- predict(trained_model, test_data, type = "prob")[, "Poor"]
#   actuals    <- test_data[[target_variable_name]]
#   roc_obj    <- roc(response = actuals, predictor = test_probs,
#                     levels = c("NotPoor", "Poor"))
# 
#   coords_df <- coords(
#     roc_obj, "all",
#     ret = c("threshold", "sensitivity", "specificity")
#   ) %>% as.data.frame()
# 
#   # impose min specificity, pick highest sens, tie-break on specificity
#   optimal_threshold_sens <- coords_df %>%
#     filter(specificity >= min_specificity) %>%
#     slice_max(order_by = sensitivity, n = 1, with_ties = TRUE) %>%
#     slice_max(order_by = specificity, n = 1) %>%
#     pull(threshold)
# 
#   cat(paste0("Threshold for max sensitivity (spec ≥ ", min_specificity, 
#              "): ", round(optimal_threshold_sens, 4), "\n"))
# 
#   # apply threshold
#   test_preds <- factor(
#     ifelse(test_probs >= optimal_threshold_sens, "Poor", "NotPoor"),
#     levels = c("NotPoor", "Poor")
#   )
#   cm <- confusionMatrix(test_preds, actuals, positive = "Poor")
# 
#   # store ROC data
#   all_roc_data_sens[[model_name]] <<- data.frame(
#     FPR   = 1 - roc_obj$specificities,
#     TPR   = roc_obj$sensitivities,
#     Model = model_name
#   )
# 
#   # store performance metrics
#   all_model_performance_metrics_sens[[model_name]] <<- list(
#     Accuracy        = cm$overall["Accuracy"],
#     Sensitivity     = cm$byClass["Sensitivity"],
#     Specificity     = cm$byClass["Specificity"],
#     Precision       = cm$byClass["Precision"],
#     F1_Score        = cm$byClass["F1"],
#     AUC             = as.numeric(auc(roc_obj)),
#     Best_Threshold  = optimal_threshold_sens
#   )
# 
#   # store params
#   if (!is.null(trained_model$bestTune)) {
#     param_str <- paste(names(trained_model$bestTune),
#                        trained_model$bestTune,
#                        sep = "=", collapse = ", ")
#   } else param_str <- "—"
#   all_model_parameters_sens[[model_name]] <<- param_str
# 
#   cat(paste0("Evaluation for ", model_name, " (sens-focus) completed.\n"))
#   return(trained_model)
# }
# 
# 
# # -------------------------------------------------------------------
# # Function: Create comparison table for sensitivity-focused models
# # -------------------------------------------------------------------
# create_comparison_table_sens <- function() {
#   if (length(all_model_performance_metrics_sens) == 0) {
#     cat("No sensitivity-focused metrics available.\n")
#     return(NULL)
#   }
# 
#   df_summary_sens <- lapply(names(all_model_performance_metrics_sens), function(m) {
#     mtrs  <- all_model_performance_metrics_sens[[m]]
#     params<- all_model_parameters_sens[[m]]
#     data.frame(
#       Model           = m,
#       Accuracy        = round(mtrs$Accuracy, 4),
#       F1_Score        = round(mtrs$F1_Score, 4),
#       AUC             = round(mtrs$AUC, 4),
#       Sensitivity     = round(mtrs$Sensitivity, 4),
#       Specificity     = round(mtrs$Specificity, 4),
#       Precision       = round(mtrs$Precision, 4),
#       Best_Threshold  = round(mtrs$Best_Threshold, 4),
#       Parameters      = params,
#       stringsAsFactors = FALSE
#     )
#   }) %>% bind_rows()
# 
#   return(df_summary_sens)
# }
```

## 4.2: Logistic Regression

```{r}
# # --- Section 4.4.1: Logistic Regression (Optimised) ---
# 
# library(caret)
# library(dplyr)
# 
# cat("\n--- Optimised Logistic Regression (NZV + Corr Filtering) ---\n")
# 
# # Set seed for reproducibility
# set.seed(123)
# 
# # 1) Start from SMOTE-balanced training data
# train_lr_opt <- train_smote_universal
# 
# # 1.1 Remove near-zero variance predictors
# nzv_idx <- nearZeroVar(train_lr_opt)
# if (length(nzv_idx) > 0) {
#   train_lr_opt <- train_lr_opt[ , -nzv_idx]
#   cat("Removed", length(nzv_idx), "NZV predictors.\n")
# } else {
#   cat("No NZV predictors found.\n")
# }
# 
# # 1.2 Remove highly correlated numeric predictors (|r| > 0.9)
# numeric_vars <- train_lr_opt %>% select(where(is.numeric)) %>% names()
# if (length(numeric_vars) > 1) {
#   corr_m <- cor(train_lr_opt[ , numeric_vars])
#   high_corr <- findCorrelation(corr_m, cutoff = 0.9)
#   if (length(high_corr) > 0) {
#     removed_vars <- numeric_vars[high_corr]
#     train_lr_opt <- train_lr_opt %>% select(-all_of(removed_vars))
#     cat("Removed", length(removed_vars), "highly correlated predictors: ",
#         paste(removed_vars, collapse = ", "), "\n")
#   } else {
#     cat("No highly correlated predictors found.\n")
#   }
# } else {
#   cat("Not enough numeric predictors to check correlation.\n")
# }
# 
# # 2) Align test data
# final_preds <- setdiff(names(train_lr_opt), "MISKIN_KAKO")
# test_lr_opt <- test_data_universal %>%
#   select(all_of(final_preds)) %>%
#   mutate(MISKIN_KAKO = test_data_universal$MISKIN_KAKO)
# 
# # 3) Train and evaluate
# model_lr_opt <- train_and_evaluate_model(
#   train_data           = train_lr_opt,
#   test_data            = test_lr_opt,
#   target_variable_name = "MISKIN_KAKO",
#   model_name           = "Logistic Regression (opt)",
#   method_caret         = "glm",
#   model_formula        = MISKIN_KAKO ~ .
# )
# 
# cat("\nOptimised Logistic Regression complete.\n")
# 
# # 4) Summary output only (plots and table saved globally)
# df_summary <- create_comparison_table()
# print(df_summary)
# 
# plot_cv_accuracy_boxplot()
# plot_all_roc_curves()
```

```{r}
# # Pastikan package yang dibutuhkan sudah terinstal dan dimuat
# # install.packages(c("caret", "ggplot2", "dplyr", "tibble"))
# 
# library(caret)
# library(ggplot2)
# library(dplyr)
# library(tibble)
# 
# # --- Anggap 'model_lr_opt' adalah model Anda yang sudah dilatih ---
# # model_lr_opt <- # ... hasil dari fungsi train_and_evaluate_model Anda
# 
# # Langkah 1: Ekstrak feature importance dari model
# importance_object <- varImp(model_lr_opt, scale = FALSE)
# 
# # Langkah 2: Siapkan data untuk plotting
# # Ubah menjadi data frame, tambahkan nama fitur sebagai kolom, dan ambil 20 teratas
# importance_df <- importance_object$importance %>%
#   as.data.frame() %>%
#   rownames_to_column(var = "Fitur") %>%
#   rename(Importance = Overall) %>%
#   arrange(desc(Importance)) %>%
#   top_n(20) # Ambil 20 fitur paling berpengaruh
# 
# cat("Top 20 Fitur Paling Berpengaruh:\n")
# print(importance_df)
# 
# 
# # Langkah 3: Buat bar chart menggunakan ggplot2
# ggplot(importance_df, aes(x = Importance, y = reorder(Fitur, Importance))) +
#   geom_bar(stat = "identity", fill = "steelblue") +
#   labs(
#     title = "Top 20 Fitur Paling Berpengaruh terhadap MISKIN_KAKO",
#     subtitle = "Berdasarkan Model Regresi Logistik",
#     x = "Tingkat Kepentingan (Importance)",
#     y = "Nama Fitur"
#   ) +
#   theme_minimal() +
#   theme(
#     plot.title = element_text(face = "bold"),
#     axis.text.y = element_text(size = 9)
#   )
```

### Variable Importance Summary

```{r}
# # --- Section 4.5.2: Distribution of Top 4 Predictors by MISKIN_KAKO ---
# 
# library(dplyr)
# library(ggplot2)
# library(scales)
# 
# # 1) Prepare the data for plotting
# plot_df <- train_lr_opt %>%
#   mutate(
#     # ensure the target is a factor
#     MISKIN_KAKO = factor(MISKIN_KAKO, levels = c(0, 1),
#                          labels = c("NotPoor", "Poor")),
#     # convert the two dummy vars to factors with readable labels
#     R2001H_No = factor(R2001H_factor.No, levels = c(0, 1),
#                        labels = c("Yes", "No")),
#     R1808_Ceramic = factor(R1808_factor.Ceramic.tiles, levels = c(0, 1),
#                            labels = c("Other", "Ceramic Tiles"))
#   )
# 
# # 2) Boxplot: Number of children by poverty status
# ggplot(plot_df, aes(x = MISKIN_KAKO, y = n_children, fill = MISKIN_KAKO)) +
#   geom_boxplot(outlier.shape = 21, outlier.fill = "white") +
#   labs(
#     title = "Distribution of Number of Children by Poverty Status",
#     x = "Poverty Status",
#     y = "Number of Children"
#   ) +
#   theme_minimal() +
#   theme(legend.position = "none")
# 
# # 3) Boxplot: Proportion of current smokers by poverty status
# ggplot(plot_df, aes(x = MISKIN_KAKO, y = prop_current_smokers, fill = MISKIN_KAKO)) +
#   geom_boxplot(outlier.shape = 21, outlier.fill = "white") +
#   labs(
#     title = "Distribution of Proportion of Current Smokers by Poverty Status",
#     x = "Poverty Status",
#     y = "Proportion of Current Smokers"
#   ) +
#   theme_minimal() +
#   theme(legend.position = "none")
# 
# # 4) Proportion bar plot: R2001H (No) by poverty status
# ggplot(plot_df, aes(x = R2001H_No, fill = MISKIN_KAKO)) +
#   geom_bar(position = "fill") +
#   scale_y_continuous(labels = percent_format()) +
#   labs(
#     title = "Proportion of Households with R2001H='No', by Poverty Status",
#     x = "R2001H = No",
#     y = "Proportion"
#   ) +
#   theme_minimal()
# 
# # 5) Proportion bar plot: R1808 (Ceramic tiles) by poverty status
# ggplot(plot_df, aes(x = R1808_Ceramic, fill = MISKIN_KAKO)) +
#   geom_bar(position = "fill") +
#   scale_y_continuous(labels = percent_format()) +
#   labs(
#     title = "Proportion of Households with Ceramic Tile Flooring, by Poverty Status",
#     x = "Flooring = Ceramic Tiles",
#     y = "Proportion"
#   ) +
#   theme_minimal()
```

```{r}
# str(train_lr_opt$R2001H_factor.No)
# # unique(train_lr_opt$R2001H_factor.No)
# 
# str(train_lr_opt$R1808_factor.Ceramic.tiles)
# # unique(train_lr_opt$R1808_factor.Ceramic.tiles)
```

```{r}
# sum(train_lr_opt$R2001H_factor.No  %in% c(0,1))
# sum(train_lr_opt$R1808_factor.Ceramic.tiles %in% c(0,1))
```

## 4.3 Decision Tree

```{r}
# --- Section 4.4.2: Decision Tree Model (Final Version) ---

# 1. Load necessary libraries
# Make sure these are loaded at the start of your session.
library(caret)
library(dplyr)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(tibble)

cat("\n--- Training Decision Tree Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_dt <- train_smote_universal

nzv_cols <- nearZeroVar(train_dt, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_dt <- train_dt[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_dt_predictors <- setdiff(names(train_dt), "MISKIN_KAKO")
test_dt <- test_data_universal %>%
  select(all_of(final_dt_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_dt), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_dt), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# Specify the complexity parameter ('cp') values to test during cross-validation.
dt_tune_grid <- expand.grid(cp = c(0.001, 0.005, 0.01, 0.02, 0.05))

# 5. Train and Evaluate the Model
# Call our reusable function to train the model and get performance metrics.
set.seed(789) # for reproducibility
model_dt <- train_and_evaluate_model(
  train_data           = train_dt,
  test_data            = test_dt,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "Decision Tree",
  method_caret         = "rpart",
  tuneGrid             = dt_tune_grid
)

cat("\nDecision Tree model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates plots specific to the decision tree model.
plot_dt_visuals <- function(model, top_n = 20) {
  
  # -- Variable Importance Plot --
  # Extract, format, and plot the top N most important variables.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#2ca02c", width = 0.8) +
    labs(
      title = paste("Top", top_n, "Variables - Decision Tree"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)

  # -- Decision Tree Diagram --
  # Plot the final, pruned tree structure.
  cat("\n--- Plotting the final pruned decision tree ---\n")
  rpart.plot(
    model$finalModel,
    type = 4,
    extra = 101,
    fallen.leaves = TRUE,
    cex = 0.7,
    main = "Final Pruned Decision Tree"
  )
}

# 7. Generate All Visuals and Summaries
# Create the specific plots for the Decision Tree.
plot_dt_visuals(model_dt)

# Update the overall comparison table and plots with the new model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

### 4.3.2 using sensitivity

```{r}
# # --- Section 4.4.3: Decision Tree (Sensitivity-Focus, min_spec=0.4) ---
# 
# # 1. Load necessary libraries (if not already loaded)
# library(caret)
# library(dplyr)
# library(rpart)
# library(rpart.plot)
# library(ggplot2)
# library(tibble)
# 
# cat("\n--- Sensitivity-Focused Decision Tree Model (min specificity = 0.4) ---\n")
# 
# # 2. Prepare Training Data (SMOTE + NZV filter)
# train_dt_sens <- train_smote_universal
# 
# nzv_idx <- nearZeroVar(train_dt_sens)
# if (length(nzv_idx) > 0) {
#   train_dt_sens <- train_dt_sens[, -nzv_idx]
#   cat("Removed", length(nzv_idx), "NZV predictors.\n")
# } else {
#   cat("No NZV predictors found.\n")
# }
# 
# # 3. Align Test Data
# final_dt_predictors <- setdiff(names(train_dt_sens), "MISKIN_KAKO")
# test_dt_sens <- test_data_universal %>%
#   select(all_of(final_dt_predictors), MISKIN_KAKO)
# 
# cat("Training dims:", paste(dim(train_dt_sens), collapse = " x "), "\n")
# cat("Test dims:    ", paste(dim(test_dt_sens), collapse = " x "), "\n")
# 
# # 4. Define Tuning Grid (same as F1 version)
# dt_tune_grid <- expand.grid(cp = c(0.001, 0.005, 0.01, 0.02, 0.05))
# 
# # 5. Train & Evaluate with sensitivity focus (min_spec = 0.4)
# set.seed(789)
# model_dt_sens <- train_and_evaluate_model_sens(
#   train_data           = train_dt_sens,
#   test_data            = test_dt_sens,
#   target_variable_name = "MISKIN_KAKO",
#   model_name           = "Decision Tree (sens-focus)",
#   method_caret         = "rpart",
#   tuneGrid             = dt_tune_grid,
#   min_specificity      = 0.4
# )
# 
# cat("\nSensitivity-focused Decision Tree complete.\n")
# 
# # 6. Model-Specific Visualisation (reuse existing function)
# plot_dt_visuals(model_dt_sens, top_n = 20)
# 
# # 7. Compare F1- vs Sensitivity-Optimized Results
# df_summary_f1   <- create_comparison_table()        # F1-based
# df_summary_sens <- create_comparison_table_sens()   # Sensitivity-based
# 
# cat("\n--- Comparison: F1 vs Sensitivity Thresholds ---\n")
# print(df_summary_f1)
# print(df_summary_sens)
# 
# # 8. (Optional) Re-plot diagnostics
# plot_cv_accuracy_boxplot()   # shows F1-models’ CV accuracy
# plot_all_roc_curves()        # overlays F1-models’ ROC curves
# # If you’d like to overlay sens-focus curves, you can write a similar ggplot using all_roc_data_sens
```

## 4.4 Random Forest

```{r}
# --- Section 4.4.3: Random Forest Model (Final Version) ---

# 1. Load necessary libraries
library(caret)
library(dplyr)
library(ranger) # The 'ranger' engine for Random Forest

cat("\n--- Training Random Forest Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_rf <- train_smote_universal

nzv_cols <- nearZeroVar(train_rf, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_rf <- train_rf[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_rf_predictors <- setdiff(names(train_rf), "MISKIN_KAKO")
test_rf <- test_data_universal %>%
  select(all_of(final_rf_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_rf), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_rf), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# Dynamically create a grid to test 'mtry' and 'min.node.size'.
num_predictors <- length(final_rf_predictors)
mtry_default <- floor(sqrt(num_predictors))
mtry_values <- unique(pmax(1, c(mtry_default - 2, mtry_default, mtry_default + 2)))

rf_tune_grid <- expand.grid(
  mtry = mtry_values,
  min.node.size = c(1, 5, 10),
  splitrule = "gini"
)

cat("Tuning grid created for Random Forest.\n")

# 5. Train and Evaluate the Model
# Call our reusable function, passing 'ranger'-specific arguments.
set.seed(2025) # for reproducibility
model_rf <- train_and_evaluate_model(
  train_data           = train_rf,
  test_data            = test_rf,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "Random Forest",
  method_caret         = "ranger",
  tuneGrid             = rf_tune_grid,
  importance           = 'impurity' # Specific argument for ranger
)

cat("\nRandom Forest model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates the variable importance plot for the RF model.
plot_rf_visuals <- function(model, top_n = 20) {
  
  # Extract, format, and plot the top N most important variables.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#1f77b4", width = 0.8) + # Blue fill color
    labs(
      title = paste("Top", top_n, "Variables - Random Forest"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)
  
  return(importance_df)
}

importance_df <- plot_rf_visuals(model_rf, top_n = 20)


# 7. Generate All Visuals and Summaries
# Create the specific plots for the Random Forest.
plot_rf_visuals(model_rf)

# Update the overall comparison table and plots with the new model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

```{r}
# Example: dictionary of variable descriptions
var_labels <- c(
  n_children = "Number of children",
  n_children_lt15 = "Number of children under 15",
  n_nuclear = "Number of Spouse + all children (Nuclear family)",
  R2001B_factor.No = "Household does not have Refrigerator",
  n_currently_school = "Number of currently in school (sum(R610 == 2))",
  edu_lowest_group.Tidak.belum.pernah.bersekolah = "Education: never attended school",
  R1817_factor.Firewood = "Main cooking fuel: firewood",
  head_mobile_owner_factor.Head.owns.a.mobile = "Head owns a mobile phone",
  prop_uses_fin_service = "Proportion using financial services",
  R2001H_factor.No = "Household does not have Motorcycle",
  all_adults_mobile_owner_factor.All.adults.own.a.mobile = "All adults own a mobile phone",
  head_fin_service_factor.Head.uses.services = "Head uses financial services",
  R1807_factor.Wood.Planks = "Main wall material: wood planks",
  head_saving_factor.Head.has.savings = "Head has savings",
  n_school = "Number in school (sum(R704 == 2))",
  prop_with_saving = "Proportion with savings",
  R2203_factor.No = "No health insurance",
  prop_adult_smp_plus = "Proportion of adults with ≥ junior high education",
  prop_current_smokers = "Proportion of current smokers",
  avg_age = "Average age of household members"
)

importance_df <- importance_df %>%
  mutate(Description = var_labels[Variable]) %>%
  select(Variable, Description, Importance)

```

```{r}
# Libraries
library(dplyr)
library(ggplot2)
library(stringr)
library(scales)

# --- Build a tidy plotting dataframe (uses Description if available) ---
make_importance_plot_data <- function(importance_df, top_n = 20) {
  importance_df %>%
    mutate(
      # fallback to Variable if Description is missing
      Label = ifelse(!is.null(Description) & !is.na(Description) & Description != "",
                     Description, Variable),
      Importance = as.numeric(Importance)
    ) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n) %>%
    # Wrap long labels for neatness and add ranking numbers
    mutate(
      Label_wrapped = str_wrap(Label, width = 40),
      Label_ranked  = paste0(row_number(), ". ", Label_wrapped)
    )
}

# --- Plot function: clean horizontal bar chart with values ---
plot_importance <- function(importance_df, top_n = 20, title_suffix = "Random Forest") {
  df_plot <- make_importance_plot_data(importance_df, top_n = top_n)

  p <- ggplot(df_plot, aes(x = Importance, y = reorder(Label_ranked, Importance))) +
    geom_col(width = 0.75) +
    geom_text(aes(label = sprintf("%.1f", Importance)),
              hjust = -0.1, size = 3) +
    scale_x_continuous(expand = expansion(mult = c(0, 0.12))) +
    labs(
      title = paste0("Top ", top_n, " Variable Importance — ", title_suffix),
      x = "Importance (scaled)",
      y = NULL,
      caption = "Notes: Importance from caret::varImp (ranger), scaled. Higher indicates greater influence."
    ) +
    theme_minimal(base_size = 11) +
    theme(
      panel.grid.minor = element_blank(),
      panel.grid.major.y = element_blank(),
      axis.text.y = element_text(size = 9),
      plot.title = element_text(face = "bold")
    ) +
    coord_cartesian(clip = "off")  # allow value labels to extend past the axis

  print(p)
  invisible(p)
}

# --- Usage ---
# Ensure `importance_df` has columns: Variable, Description (optional), Importance
plot_importance(importance_df, top_n = 20, title_suffix = "Random Forest")

# Optional: save a high-res figure for your report
ggsave("rf_importance_top20.png", width = 7.5, height = 6.0, dpi = 300)

```

```{r}
# Libraries
library(dplyr)
library(ggplot2)
library(stringr)
library(scales)

# --- Prep: tidy data for plotting (uses Description where available) ---
make_importance_plot_data2 <- function(importance_df, top_n = 20) {
  importance_df %>%
    mutate(
      Label = ifelse(!is.null(Description) & !is.na(Description) & Description != "",
                     Description, Variable),
      Importance = as.numeric(Importance)
    ) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n) %>%
    mutate(
      Label_wrapped = str_wrap(Label, width = 40),
      Label_ranked  = paste0(row_number(), ". ", Label_wrapped)
    )
}

# --- Lollipop plot: academic, crisp, and compact ---
plot_importance_lollipop <- function(importance_df, top_n = 20, title_suffix = "Random Forest") {
  df_plot <- make_importance_plot_data2(importance_df, top_n = top_n)

  ggplot(df_plot, aes(x = Importance, y = reorder(Label_ranked, Importance))) +
    # stems
    geom_segment(aes(x = 0, xend = Importance, yend = reorder(Label_ranked, Importance)),
                 linewidth = 0.5) +
    # dots
    geom_point(size = 2) +
    # value labels at the end of sticks
    geom_text(aes(label = sprintf("%.1f", Importance)),
              hjust = -0.2, size = 3) +
    scale_x_continuous(expand = expansion(mult = c(0, 0.15))) +
    labs(
      title = paste0("Top ", top_n, " Variable Importance — ", title_suffix),
      x = "Importance (scaled)",
      y = NULL,
      caption = "Notes: Importance from caret::varImp (ranger), scaled. Higher indicates greater influence."
    ) +
    theme_classic(base_size = 11) +
    theme(
      axis.text.y = element_text(size = 9),
      plot.title  = element_text(face = "bold"),
      panel.grid  = element_blank()
    ) +
    coord_cartesian(clip = "off")
}

# --- Usage ---
plot_importance_lollipop(importance_df, top_n = 20, title_suffix = "Random Forest")

# Optional: save a high-res figure
ggsave("rf_importance_top20_lollipop.png", width = 7.5, height = 6.0, dpi = 300)

```

```{r}
# --- Fast SHAP Summary for RF via DALEX ---

# 1) Install if needed
# install.packages("DALEX")

library(DALEX)
library(dplyr)
library(ggplot2)

# 2) Prepare a small test set for speed
set.seed(42)
X_full  <- test_rf %>% select(-MISKIN_KAKO)
y_full  <- test_rf$MISKIN_KAKO
idx     <- sample(nrow(X_full), 200)
X_small <- X_full[idx, ]
y_small <- y_full[idx]

# 3) Define a predict function matching DALEX’s expectations
predict_rf <- function(model, newdata) {
  # model is the ranger::ranger object
  preds <- predict(model, data = newdata, type = "response")$predictions
  preds[, "Poor"]
}

# 4) Create the DALEX explainer
explainer_rf <- explain(
  model          = model_rf$finalModel,
  data           = X_small,
  y              = y_small,
  predict_function = predict_rf,
  label          = "Random Forest"
)

# 5) Compute SHAP‐style breakdown (“type = 'shap'”) with B=10
set.seed(123)
shap_parts <- predict_parts(
  explainer       = explainer_rf,
  new_observation = X_small,
  type            = "shap",
  B               = 10
)

# 6) Plot the aggregated SHAP summary
plot(shap_parts) +
  ggtitle("Approximate SHAP Summary (Top Features) for Random Forest") +
  theme_minimal()
```

```{r}
# --- Section 4.5.7b: SHAP Summary — Top 15 Features Only for RF via DALEX ---

library(dplyr)
library(ggplot2)

# Assume shap_parts was created as in the previous block:
# shap_parts <- predict_parts(..., type = "shap", B = 10)

# 1) Identify the top 15 features by mean absolute SHAP contribution
top15_rf <- shap_parts %>%
  filter(variable != "_baseline_") %>%             # drop baseline
  group_by(variable) %>%
  summarise(mean_abs = mean(abs(contribution))) %>%
  arrange(desc(mean_abs)) %>%
  slice_head(n = 15) %>%
  pull(variable)

# 2) Filter the SHAP data to only those top 15
shap_parts_top15 <- shap_parts %>%
  filter(variable %in% top15_rf)

# 3) Plot the aggregated SHAP summary for just these features
plot(shap_parts_top15) +
  ggtitle("Approximate SHAP Summary — Top 15 Features for Random Forest") +
  theme_minimal()
```

```{r}
# --- Section 4.5.8: SHAP Summary Beeswarm — Top 15 Features for RF ---

library(fastshap)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggbeeswarm)
library(viridis)  # for a nice continuous color scale

# 1) Subsample 200 rows for speed
set.seed(42)
X_full   <- test_rf %>% select(-MISKIN_KAKO)
sample_i <- sample(nrow(X_full), 1000)
X_samp   <- X_full[sample_i, ]

# 2) Wrapper to get P(Poor) from ranger model
pred_fun_rf <- function(object, newdata) {
  predict(object, data = newdata, type = "response")$predictions[, "Poor"]
}

# 3) Calculate fastshap SHAP values (nsim=10 for speed)
set.seed(123)
shap_mat <- fastshap::explain(
  object       = model_rf$finalModel,
  X            = X_samp,
  pred_wrapper = pred_fun_rf,
  nsim         = 10
)

# 4) Pick top 15 features by mean absolute SHAP
mean_abs <- apply(abs(shap_mat), 2, mean)
top15    <- names(sort(mean_abs, decreasing = TRUE))[1:15]

# 5) Pivot SHAP to long form and attach feature values
shap_long <- as_tibble(shap_mat[, top15]) %>%
  mutate(id = row_number()) %>%
  pivot_longer(-id, names_to = "Feature", values_to = "SHAP") %>%
  left_join(
    X_samp %>% mutate(id = row_number()) %>% pivot_longer(-id, names_to = "Feature", values_to = "Value"),
    by = c("id","Feature")
  )

# 6) Force the factor order of the features
shap_long$Feature <- factor(shap_long$Feature, levels = top15)

# 7) Plot the beeswarm SHAP summary
ggplot(shap_long, aes(x = SHAP, y = Feature, color = Value)) +
  geom_quasirandom(alpha = 0.6, width = 0.3, size = 1.8) +
  scale_color_viridis(option = "C", name = "Feature value") +
  labs(
    title = "SHAP Summary Plot — Top 15 Features (Random Forest)",
    x     = "SHAP value (impact on P(Poor))",
    y     = "Feature"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "bottom",
    legend.key.width = unit(2, "cm")
  )
```

```{r}
# Assuming you have the long SHAP data in `shap_long` (with columns `Feature` and `SHAP`)
library(dplyr)
library(tidyr)

shap_side_counts <- shap_long %>%
  # Only keep the top‐15 features you plotted
  filter(Feature %in% top15) %>%
  # Classify each dot as left (neg), right (pos) or zero
  mutate(side = case_when(
    SHAP <  0 ~ "Left (SHAP < 0)",
    SHAP >  0 ~ "Right (SHAP > 0)",
    TRUE      ~ "Zero"
  )) %>%
  # Count per feature × side
  count(Feature, side) %>%
  # Spread into wide format
  pivot_wider(names_from = side, values_from = n, values_fill = 0) %>%
  # (Optionally) add a total column
  mutate(Total = `Left (SHAP < 0)` + `Right (SHAP > 0)` + `Zero`)

print(shap_side_counts)
```

```{r}
library(dplyr)

# shap_long: your data.frame with columns Feature, SHAP, Value
# For a binary/categorical feature, Value will be 0/1 or similar;
# for numeric, it's continuous.

# 1) For each point, classify high vs low by median split for numeric,
#    or by level for binary (0=low, 1=high).
shap_with_level <- shap_long %>%
  group_by(Feature) %>%
  mutate(
    # For numeric: high if above median, else low; for binary this matches 1 vs 0
    level = ifelse(
      is.numeric(Value),
      ifelse(Value > median(Value, na.rm = TRUE), "High", "Low"),
      ifelse(Value == max(Value, na.rm = TRUE), "High", "Low")
    ),
    side = case_when(
      SHAP >  0 ~ "Right",
      SHAP <  0 ~ "Left",
      TRUE      ~ "Zero"
    )
  ) %>%
  ungroup()

# 2) Tabulate for each feature how many Right‐High, Right‐Low, Left‐High, Left‐Low
check_table <- shap_with_level %>%
  filter(side %in% c("Right","Left")) %>%
  count(Feature, side, level) %>%
  tidyr::pivot_wider(
    names_from  = c(side, level),
    values_from = n,
    values_fill = 0
  ) %>%
  # Compute total Right and total Left for sanity
  mutate(
    Total_Right = Right_High + Right_Low,
    Total_Left  = Left_High  + Left_Low
  ) %>%
  arrange(desc(Total_Right + Total_Left))

print(check_table)
```

## 4.5 XGBoost

```{r}
# --- Section 4.4.4: XGBoost Model (Final Version) ---

# 1. Load necessary libraries
# Make sure these are loaded at the start of your session.
library(caret)
library(dplyr)
library(xgboost) # The 'xgboost' engine
library(ggplot2)

cat("\n--- Training XGBoost Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_xgb <- train_smote_universal

nzv_cols <- nearZeroVar(train_xgb, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_xgb <- train_xgb[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_xgb_predictors <- setdiff(names(train_xgb), "MISKIN_KAKO")
test_xgb <- test_data_universal %>%
  select(all_of(final_xgb_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_xgb), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_xgb), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# As you specified, we will tune nrounds, max_depth, and eta.
xgb_tune_grid <- expand.grid(
  nrounds = c(100, 200),
  max_depth = c(3, 6),
  eta = c(0.1, 0.3),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

cat("Tuning grid created for XGBoost.\n")

# 5. Train and Evaluate the Model
# Call our reusable function to train the model and get performance metrics.
set.seed(303) # for reproducibility
model_xgb <- train_and_evaluate_model(
  train_data           = train_xgb,
  test_data            = test_xgb,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "XGBoost",
  method_caret         = "xgbTree",
  tuneGrid             = xgb_tune_grid,
  verbose              = FALSE # Suppress XGBoost's verbose output
)

cat("\nXGBoost model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates the variable importance plot for the XGBoost model.
plot_xgb_visuals <- function(model, top_n = 20) {
  
  # Extract, format, and plot the top N most important variables using caret's method.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#d62728", width = 0.8) + # Red fill color
    labs(
      title = paste("Top", top_n, "Variables - XGBoost"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)
}

# 7. Generate All Visuals and Summaries
# Create the specific plots for XGBoost.
plot_xgb_visuals(model_xgb)

# Update the overall comparison table and plots with the final model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

```{r}
# --- Section 4.5.9: XGBoost SHAP Beeswarm & Tabulation (Top 15 Features) ---

# 0) Load required packages
library(SHAPforxgboost)
library(dplyr)
library(tidyr)
library(ggplot2)

# 1) Prepare test set as a matrix (drop the target)
X_test <- test_xgb %>%
  select(-MISKIN_KAKO) %>%
  as.matrix()

# 2) Compute SHAP values for the full hold-out set
shap_vals  <- shap.values(xgb_model = model_xgb$finalModel, X_train = X_test)
shap_score <- shap_vals$shap_score      # matrix of SHAP contributions
mean_shap  <- shap_vals$mean_shap_score # named vector of mean |SHAP| per feature

# 3) Convert SHAP to long format for plotting
shap_long <- shap.prep(shap_contrib = shap_score, X_train = X_test)

# 4) Identify the top 15 features by importance
top15 <- names(sort(mean_shap, decreasing = TRUE))[1:15]

# 5) Filter the long SHAP data to those 15, preserving order
shap_long15 <- shap_long %>%
  filter(variable %in% top15) %>%
  mutate(variable = factor(variable, levels = top15))

# 6) Beeswarm‐style SHAP summary plot (only top 15)
shap.plot.summary(shap_long15) +
  ggtitle("XGBoost SHAP Summary — Top 15 Features") +
  theme_minimal() +
  theme(
    legend.position  = "bottom",
    legend.direction = "horizontal"
  )

# 7) Build the Left/Right × Low/High counts table

# 7a) Pivot shap_score and X_test to long form
shap_df <- as.data.frame(shap_score[, top15]) %>%
  mutate(id = row_number()) %>%
  pivot_longer(
    cols      = -id,
    names_to  = "Feature",
    values_to = "SHAP"
  )

val_df <- as.data.frame(X_test[, top15]) %>%
  mutate(id = row_number()) %>%
  pivot_longer(
    cols      = -id,
    names_to  = "Feature",
    values_to = "Value"
  )

df_long <- inner_join(shap_df, val_df, by = c("id","Feature"))

# 7b) Count Left vs Right and Low vs High per feature
shap_tab <- df_long %>%
  group_by(Feature) %>%
  mutate(
    # High/Low by median split of the feature’s test set distribution
    level = ifelse(Value > median(Value, na.rm = TRUE), "High", "Low"),
    # Side by SHAP sign
    side  = ifelse(SHAP >  0, "Right", "Left")
  ) %>%
  count(Feature, side, level) %>%
  pivot_wider(
    names_from  = c(side, level),
    values_from = n,
    values_fill = 0
  ) %>%
  mutate(
    Total_Right = Right_High + Right_Low,
    Total_Left  = Left_High  + Left_Low
  ) %>%
  ungroup()

# 8) Print the summary table
print(shap_tab)
```

```{r}
# --- Section 4.5.9: XGBoost SHAP Beeswarm & Tabulation (Top 15 Features) ---

library(SHAPforxgboost)
library(dplyr)
library(tidyr)
library(ggplot2)

# 1) Prepare test set matrix
X_test <- test_xgb %>%
  select(-MISKIN_KAKO) %>%
  as.matrix()

# 2) Compute SHAP values
shap_out   <- shap.values(xgb_model = model_xgb$finalModel, X_train = X_test)
shap_score <- shap_out$shap_score
mean_shap  <- shap_out$mean_shap_score

# 3) Long‐format SHAP for plotting
shap_long <- shap.prep(shap_contrib = shap_score, X_train = X_test)

# 4) Top 15 features
top15 <- names(sort(mean_shap, decreasing = TRUE))[1:15]

# 5) Filter for plot
shap_long15 <- shap_long %>%
  filter(variable %in% top15) %>%
  mutate(variable = factor(variable, levels = top15))

# 6) Beeswarm summary plot
shap.plot.summary(shap_long15) +
  ggtitle("XGBoost SHAP Summary — Top 15 Features") +
  theme_minimal() +
  theme(
    legend.position  = "bottom",
    legend.direction = "horizontal"
  )

# 7a) Build long SHAP table for counts
#    - First coerce to a plain data.frame
df_shap_raw <- as.data.frame(shap_score)
shap_df <- df_shap_raw %>%
  select(all_of(top15)) %>%
  mutate(id = row_number()) %>%
  pivot_longer(
    cols      = -id,
    names_to  = "Feature",
    values_to = "SHAP"
  )

df_val_raw <- as.data.frame(X_test)
val_df <- df_val_raw %>%
  select(all_of(top15)) %>%
  mutate(id = row_number()) %>%
  pivot_longer(
    cols      = -id,
    names_to  = "Feature",
    values_to = "Value"
  )

df_long <- inner_join(shap_df, val_df, by = c("id", "Feature"))

# 7b) Tabulate Left/Right × Low/High
shap_tab <- df_long %>%
  group_by(Feature) %>%
  mutate(
    level = ifelse(Value > median(Value, na.rm = TRUE), "High", "Low"),
    side  = ifelse(SHAP  >  0, "Right", "Left")
  ) %>%
  count(Feature, side, level) %>%
  pivot_wider(
    names_from   = c(side, level),
    values_from  = n,
    values_fill  = 0
  ) %>%
  mutate(
    Total_Right = Right_High + Right_Low,
    Total_Left  = Left_High  + Left_Low
  ) %>%
  ungroup()

# 8) View summary table
print(shap_tab)
```

# Section 5: Export Comparison

```{r}
# # create an output directory (optional)
# output_dir <- "rds_outputs"
# if (!dir.exists(output_dir)) dir.create(output_dir)
# 
# # 1. Comparison table
# comparison_tbl <- create_comparison_table()
# saveRDS(
#   comparison_tbl,
#   file = file.path(output_dir, "comparison_table_HH_Ignorance.rds")
# )
# 
# # 2. CV‐accuracy boxplot data
# cv_accuracy_df <- bind_rows(all_cv_accuracy_results) %>% 
#   filter(is.finite(Accuracy))
# saveRDS(
#   cv_accuracy_df,
#   file = file.path(output_dir, "cv_accuracy_data_HH_Ignorance.rds")
# )
# 
# # 3. ROC curve data
# roc_data_df <- bind_rows(all_roc_data)
# saveRDS(
#   roc_data_df,
#   file = file.path(output_dir, "roc_curve_data_HH_Ignorance.rds")
# )
# 
# cat("✔️  Three RDS files written to", output_dir, "\n",
#     "- comparison_table_HH_Ignorance.rds\n",
#     "- cv_accuracy_data_HH_Ignorance.rds\n",
#     "- roc_curve_data_HH_Ignorance.rds\n")
```

---
title: "Predicting Ignorance New"
author: "Ahmad Risal"
date: "2025-06-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 0: Initial Data Preparation

```{r}
df_model_raw <- readRDS("final_analysis_data.rds")
```

```{r}
# Identify variables with more than one class
multi_class_vars <- sapply(df_model_raw, function(x) length(class(x)) > 1)

# Show variable names and their class attributes
multi_class_details <- sapply(df_model_raw[multi_class_vars], class)

# Display the result
multi_class_details
```

```{r}
# Identify variables with both "ordered" and "factor"
multi_class_vars <- sapply(df_model_raw, function(x) all(c("ordered", "factor") %in% class(x)))

# Forcefully convert all ordered factors to plain factors
df_model_raw[multi_class_vars] <- lapply(
  df_model_raw[multi_class_vars],
  function(x) factor(as.character(x))
)
```

```{r}
# Check updated classes
sapply(df_model_raw[multi_class_vars], class)
```

```{r}
# Identify character columns
char_vars <- sapply(df_model_raw, is.character)

# Convert them to factors
df_model_raw[char_vars] <- lapply(df_model_raw[char_vars], as.factor)
```

```{r}
table(sapply(df_model_raw, class))
```

```{r}
# Load required libraries
library(dplyr)
library(caret)

# 2. Standardise column names
names(df_model_raw) <- make.names(names(df_model_raw), unique = TRUE)
cat("Column names standardised.\n")

# 3. Clean factor levels by replacing spaces with dots
df_model_raw <- df_model_raw %>%
  mutate(across(where(is.factor), ~ {
    levels(.) <- make.names(levels(.), unique = FALSE)
    .
  }))
cat("Factor levels cleaned using make.names().\n")

# 4. Ensure MISKIN_KAKO is a factor with correct labels
# Level order: 0 = TIDAK.MISKIN (reference), 1 = MISKIN (positive)
df_model_raw <- df_model_raw %>%
  mutate(MISKIN_KAKO = factor(MISKIN_KAKO, levels = c(0, 1), labels = c("TIDAK.MISKIN", "MISKIN")))
cat("MISKIN_KAKO formatted as factor with labels.\n")

# 5. Check for missing values
na_summary <- sapply(df_model_raw, function(x) sum(is.na(x)))
na_summary_display <- na_summary[na_summary > 0]
if(length(na_summary_display) > 0) {
  cat("\nVariables with missing values:\n")
  print(na_summary_display)
} else {
  cat("\nNo missing values found.\n")
}

# 6. Create dummy variables
categorical_features <- names(df_model_raw)[sapply(df_model_raw, is.factor)]
categorical_features <- setdiff(categorical_features, "MISKIN_KAKO") # Exclude target

if (length(categorical_features) > 0) {
  # Create dummy variables, dropping one level per factor to avoid multicollinearity
  dummy_model <- dummyVars(~ ., data = df_model_raw[, categorical_features], fullRank = TRUE)
  # ---------------------------------
  
  encoded_features <- predict(dummy_model, newdata = df_model_raw[, categorical_features])
  
  # Convert to data.frame to ensure compatibility
  encoded_features <- as.data.frame(encoded_features)

  # Bind encoded features with remaining (non-categorical + target)
  data_ml <- df_model_raw %>%
    select(-one_of(categorical_features)) %>% # remove original factor columns
    bind_cols(encoded_features)
} else {
  data_ml <- df_model_raw
}

# 7. Final structure and summary
cat("\nStructure of final modelling dataset (data_ml):\n")
print(str(data_ml))
cat("\nPreview column names:\n")
print(head(names(data_ml)))
cat("\nClass balance:\n")
print(table(data_ml$MISKIN_KAKO))

```

# Section 1: Universal Train and Test Split

```{r}
library(caret)
library(dplyr)

# 1. Convert MISKIN_KAKO to numeric (0 = TIDAK.MISKIN, 1 = MISKIN)
data_ml <- data_ml %>%
  mutate(MISKIN_KAKO = as.integer(MISKIN_KAKO == "MISKIN"))
cat("MISKIN_KAKO converted to numeric (0 = TIDAK.MISKIN, 1 = MISKIN).\n")

# 2. Set random seed for reproducibility
set.seed(123)

# 3. Stratified sampling (75% train, 25% test)
train_index_universal <- createDataPartition(
  y = data_ml$MISKIN_KAKO,
  p = 0.75,
  list = FALSE
)

# 4. Split into training and testing datasets
train_data_universal <- data_ml[train_index_universal, ]
test_data_universal  <- data_ml[-train_index_universal, ]

# 5. Summary of split
cat("\nDimensions of train_data_universal:\n")
print(dim(train_data_universal))
cat("Class balance in train_data_universal:\n")
print(prop.table(table(train_data_universal$MISKIN_KAKO)))

cat("\nDimensions of test_data_universal:\n")
print(dim(test_data_universal))
cat("Class balance in test_data_universal:\n")
print(prop.table(table(test_data_universal$MISKIN_KAKO)))
```

# **Section 2: Apply SMOTE on Universal Training Data**

```{r}
# Load required libraries
library(dplyr)
library(themis)

cat("Step 2: Applying SMOTE on the universal training data for a 60:40 balance...\n")

# --- 2.1 Show class distribution BEFORE SMOTE ---
cat("\nClass distribution BEFORE SMOTE:\n")
print(table(train_data_universal$MISKIN_KAKO))
cat("\nProportions BEFORE SMOTE:\n")
print(prop.table(table(train_data_universal$MISKIN_KAKO)))

# --- 2.2 Apply SMOTE using themis ---
# Themis needs the target variable to be a factor
train_data_for_smote <- train_data_universal %>%
  mutate(MISKIN_KAKO = factor(MISKIN_KAKO, levels = c(0, 1), labels = c("TIDAK.MISKIN", "MISKIN")))

# Target ratio: 40% minority (MISKIN) and 60% majority (TIDAK.MISKIN)
target_ratio <- 40 / 60 

# Apply SMOTE, explicitly calling the function from the themis package
set.seed(123)
train_smote_universal <- themis::smote(
  train_data_for_smote, 
  var = "MISKIN_KAKO",
  over_ratio = target_ratio
)

# Convert target variable back to numeric (0/1) for modelling
train_smote_universal <- train_smote_universal %>%
  mutate(MISKIN_KAKO = as.integer(MISKIN_KAKO == "MISKIN"))
  
# --- 2.3 Show class distribution AFTER SMOTE ---
cat("\nClass distribution AFTER SMOTE:\n")
print(table(train_smote_universal$MISKIN_KAKO))
cat("\nProportions AFTER SMOTE:\n")
print(prop.table(table(train_smote_universal$MISKIN_KAKO)))

cat("\nSMOTE application complete. The dataset 'train_smote_universal' is ready.\n")
```

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# 1. Prepare data for plotting
# Create a dataframe for the original data
df_before <- data.frame(
  MISKIN_KAKO = train_data_universal$MISKIN_KAKO, # Data before SMOTE
  Source = "Before SMOTE"
)

# Create a dataframe for the balanced data
df_after <- data.frame(
  MISKIN_KAKO = train_smote_universal$MISKIN_KAKO, # Data after SMOTE
  Source = "After SMOTE"
)

# 2. Combine the two dataframes
df_combined <- bind_rows(df_before, df_after)

# 3. Convert the target variable to a descriptive factor for better labels
df_combined <- df_combined %>%
  mutate(MISKIN_KAKO_label = factor(MISKIN_KAKO,
                             levels = c(0, 1),
                             labels = c("Not Poor", "Poor")))

# 4. Create the bar chart
ggplot(df_combined, aes(x = MISKIN_KAKO_label, fill = Source)) +
  geom_bar(position = "dodge") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)), # Automatically adds the count as a label
    position = position_dodge(width = 0.9),
    vjust = -0.5, # Adjust vertical position to be above the bar
    size = 4
  ) +
  scale_fill_manual(values = c("Before SMOTE" = "#1f77b4", "After SMOTE" = "#ff7f0e")) +
  labs(
    title = "Class Distribution Before and After SMOTE",
    x = "Poverty Status (MISKIN_KAKO)",
    y = "Number of Samples",
    fill = "Dataset"
  ) +
  theme_minimal(base_size = 14)
```

# Section 4: Modelling

## 4.1: Reusable Functions

```{r}
# ===================================================================
# Final Reusable Functions for Modelling & Evaluation
# ===================================================================

# 1. Load All Required Libraries
# -------------------------------------------------------------------
library(caret)
library(pROC)
library(dplyr)
library(tidyr)
library(ggplot2)
library(purrr)
library(tibble)
library(readr)
library(writexl)
library(doParallel)


# 2. Setup Parallel Processing & Global Result Lists
# -------------------------------------------------------------------
# Use n-1 cores to leave one free for system processes
num_cores <- detectCores() - 1
if (num_cores < 1) num_cores <- 1
registerDoParallel(cores = num_cores)
cat(paste0("Registered ", num_cores, " cores for parallel processing.\n"))

# Initialise global lists to store results from all models
all_model_performance_metrics <- list()
all_cv_accuracy_results <- list()
all_roc_data <- list()
all_model_parameters <- list()

cat("Global result lists created.\n")


# 3. Reusable Function Definitions
# -------------------------------------------------------------------

# --- Function 1: Train and Evaluate a Model ---
train_and_evaluate_model <- function(
  train_data,
  test_data,
  target_variable_name,
  model_name,
  method_caret,
  model_formula   = NULL,
  tuneGrid        = NULL,
  preProc_methods = NULL,
  ...
) {
  cat(paste0("\n--- Training and Evaluating ", model_name, " Model ---\n"))

  # Control for 5-fold CV optimising ROC
  fitControl <- trainControl(
    method            = "cv",
    number            = 5,
    savePredictions   = "final",
    classProbs        = TRUE,
    summaryFunction   = twoClassSummary,
    allowParallel     = TRUE
  )

  # Convert numeric binary target to factor for classification
  train_data[[target_variable_name]] <- factor(
    ifelse(train_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
    levels = c("NotPoor", "Poor")
  )
  test_data[[target_variable_name]] <- factor(
    ifelse(test_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
    levels = c("NotPoor", "Poor")
  )

  # Build formula
  formula_obj <- if (is.null(model_formula)) 
    as.formula(paste0(target_variable_name, " ~ .")) 
  else model_formula

  # Set seed and train
  set.seed(456)
  trained_model <- train(
    formula_obj,
    data      = train_data,
    method    = method_caret,
    trControl = fitControl,
    tuneGrid  = tuneGrid,
    preProcess= preProc_methods,
    metric    = "ROC",
    ...
  )
  cat(paste0(model_name, " model training completed.\n"))

  # Extract CV accuracy for diagnostic plot
  preds <- trained_model$pred
  if (!is.null(trained_model$bestTune)) {
    for (param in names(trained_model$bestTune)) {
      preds <- preds[preds[[param]] == trained_model$bestTune[[param]], ]
    }
  }
  cv_results_df <- preds %>%
    group_by(Resample) %>%
    summarize(Accuracy = mean(pred == obs, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(Model = model_name)
  all_cv_accuracy_results[[model_name]] <<- cv_results_df

  # Test set evaluation with optimal threshold for F1-Score
  test_probs <- predict(trained_model, test_data, type = "prob")[, "Poor"]
  actuals <- test_data[[target_variable_name]]
  roc_obj <- roc(response = actuals, predictor = test_probs, levels = c("NotPoor", "Poor"))
                   
  # Find optimal threshold that maximizes F1-Score
  coords_df <- coords(roc_obj, "all", ret = c("threshold", "precision", "recall")) %>%
    as.data.frame() %>%
    mutate(f1 = 2 * (precision * recall) / (precision + recall)) %>%
    filter(is.finite(f1))

  optimal_threshold <- coords_df %>%
    slice_max(order_by = f1, n = 1) %>%
    pull(threshold)

  cat(paste0("Optimal threshold found to maximise F1-Score: ", round(optimal_threshold, 4), "\n"))

  # Apply the optimal threshold for predictions
  test_preds <- factor(ifelse(test_probs >= optimal_threshold, "Poor", "NotPoor"),
                       levels = c("NotPoor", "Poor"))
  
  cm <- confusionMatrix(test_preds, actuals, positive = "Poor")

  # Store ROC curve data
  all_roc_data[[model_name]] <<- data.frame(
    FPR   = 1 - roc_obj$specificities,
    TPR   = roc_obj$sensitivities,
    Model = model_name
  )

  # Store final performance metrics
  all_model_performance_metrics[[model_name]] <<- list(
    Accuracy        = cm$overall["Accuracy"],
    Sensitivity     = cm$byClass["Sensitivity"], # Recall
    Specificity     = cm$byClass["Specificity"],
    Precision       = cm$byClass["Precision"],
    F1_Score        = cm$byClass["F1"],
    AUC             = as.numeric(auc(roc_obj)),
    Best_Threshold  = optimal_threshold
  )

  # Store tuning parameters
  if (!is.null(trained_model$bestTune)) {
    param_str <- paste(names(trained_model$bestTune), trained_model$bestTune, sep = "=", collapse = ", ")
  } else {
    param_str <- "—"
  }
  all_model_parameters[[model_name]] <<- param_str

  cat(paste0("Evaluation for ", model_name, " on test data completed.\n"))
  return(trained_model)
}


# --- Function 2: Create a Summary Comparison Table ---
create_comparison_table <- function() {
  if (length(all_model_performance_metrics) == 0) {
    cat("No model performance metrics available.\n")
    return(NULL)
  }
  
  # Safely bind model names and metrics into a data frame
  df_summary <- lapply(names(all_model_performance_metrics), function(model_name) {
    metrics <- all_model_performance_metrics[[model_name]]
    params  <- all_model_parameters[[model_name]]
    
    data.frame(
      Model           = model_name,
      Accuracy        = round(metrics$Accuracy, 4),
      F1_Score        = round(metrics$F1_Score, 4),
      AUC             = round(metrics$AUC, 4),
      Sensitivity     = round(metrics$Sensitivity, 4),
      Specificity     = round(metrics$Specificity, 4),
      Precision       = round(metrics$Precision, 4),
      Best_Threshold  = round(metrics$Best_Threshold, 4),
      Parameters      = paste(params, collapse = ", ")
    )
  }) %>%
    bind_rows()

  return(df_summary)
}


# --- Function 3: Plot Cross-Validation Accuracy Boxplot (Diagnostic) ---
plot_cv_accuracy_boxplot <- function() {
  if (length(all_cv_accuracy_results) == 0) return()
  df <- bind_rows(all_cv_accuracy_results) %>% filter(is.finite(Accuracy))
  p <- ggplot(df, aes(x = reorder(Model, Accuracy, median), y = Accuracy, fill = Model)) +
    geom_boxplot() +
    labs(
      title = "Model Stability: Cross-Validation Accuracy",
      subtitle = "Shows performance distribution during training on SMOTE data",
      x = "Model", y = "Accuracy"
    ) +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = 25, hjust = 1))
  print(p)
}


# --- Function 4: Plot Final Performance Bar Chart ---
plot_all_performance_bar_chart <- function(df, metric_to_plot = "F1_Score") {
  if (!metric_to_plot %in% names(df)) stop("Metric not found: ", metric_to_plot)
  
  p <- ggplot(df, aes_string(x = sprintf("reorder(Model, -%s)", metric_to_plot), y = metric_to_plot, fill = "Model")) +
    geom_col(width = 0.7) +
    geom_text(aes_string(label = metric_to_plot), vjust = -0.5, size = 3.5) +
    labs(
      title = paste("Final Model Performance on Test Set"),
      subtitle = paste("Metric:", metric_to_plot),
      x = "Model", y = metric_to_plot
    ) +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = 25, hjust = 1))
  print(p)
}


# --- Function 5: Plot All ROC Curves ---
plot_all_roc_curves <- function() {
  if (length(all_roc_data) == 0) return()
  df <- bind_rows(all_roc_data)
  
  # Add AUC values for the legend
  auc_labels <- create_comparison_table() %>%
    select(Model, AUC) %>%
    mutate(label = paste0(Model, " (AUC = ", AUC, ")"))
    
  df <- df %>% left_join(auc_labels, by = "Model")

  p <- ggplot(df, aes(x = FPR, y = TPR, colour = label)) +
    geom_line(linewidth = 1.1) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "grey") +
    labs(
      title = "ROC Curves for All Models on Test Set",
      x = "False Positive Rate (1 - Specificity)", 
      y = "True Positive Rate (Sensitivity)",
      colour = "Model"
    ) +
    theme_minimal()
  print(p)
}


# --- Function 6: Export Summary Table to Excel ---
export_summary_to_excel <- function(summary_df, file_path = "model_performance_summary.xlsx") {
  if (is.null(summary_df) || nrow(summary_df) == 0) {
    cat("Summary data frame is empty. Nothing to export.\n")
    return()
  }
  
  write_xlsx(summary_df, path = file_path)
  cat(paste0("\nFinal model performance summary exported to: ", file_path, "\n"))
}

```

```{r}
# Global lists to store results
all_model_performance_metrics <- list()
all_cv_accuracy_results <- list()
all_roc_data <- list()
all_model_parameters <- list()

cat("Global result lists created.\n")
```

## 4.2 Decision Tree

```{r}
# --- Section 4.2: Decision Tree Model (Final Version) ---

# 1. Load necessary libraries
# Make sure these are loaded at the start of your session.
library(caret)
library(dplyr)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(tibble)

cat("\n--- Training Decision Tree Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_dt <- train_smote_universal

nzv_cols <- nearZeroVar(train_dt, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_dt <- train_dt[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_dt_predictors <- setdiff(names(train_dt), "MISKIN_KAKO")
test_dt <- test_data_universal %>%
  select(all_of(final_dt_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_dt), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_dt), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# Specify the complexity parameter ('cp') values to test during cross-validation.
dt_tune_grid <- expand.grid(cp = c(0.001, 0.005, 0.01, 0.02, 0.05))

# 5. Train and Evaluate the Model
# Call our reusable function to train the model and get performance metrics.
set.seed(789) # for reproducibility
model_dt <- train_and_evaluate_model(
  train_data           = train_dt,
  test_data            = test_dt,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "Decision Tree",
  method_caret         = "rpart",
  tuneGrid             = dt_tune_grid
)

cat("\nDecision Tree model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates plots specific to the decision tree model.
plot_dt_visuals <- function(model, top_n = 20) {
  
  # -- Variable Importance Plot --
  # Extract, format, and plot the top N most important variables.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#2ca02c", width = 0.8) +
    labs(
      title = paste("Top", top_n, "Variables - Decision Tree"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)

  # -- Decision Tree Diagram --
  # Plot the final, pruned tree structure.
  cat("\n--- Plotting the final pruned decision tree ---\n")
  rpart.plot(
    model$finalModel,
    type = 4,
    extra = 101,
    fallen.leaves = TRUE,
    cex = 0.7,
    main = "Final Pruned Decision Tree"
  )
}

# 7. Generate All Visuals and Summaries
# Create the specific plots for the Decision Tree.
plot_dt_visuals(model_dt)

# Update the overall comparison table and plots with the new model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

## 4.3 Random Forest

```{r}
# --- Section 4.3: Random Forest Model (Final Version) ---

# 1. Load necessary libraries
library(caret)
library(dplyr)
library(ranger) # The 'ranger' engine for Random Forest

cat("\n--- Training Random Forest Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_rf <- train_smote_universal

nzv_cols <- nearZeroVar(train_rf, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_rf <- train_rf[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_rf_predictors <- setdiff(names(train_rf), "MISKIN_KAKO")
test_rf <- test_data_universal %>%
  select(all_of(final_rf_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_rf), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_rf), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# Dynamically create a grid to test 'mtry' and 'min.node.size'.
num_predictors <- length(final_rf_predictors)
mtry_default <- floor(sqrt(num_predictors))
mtry_values <- unique(pmax(1, c(mtry_default - 2, mtry_default, mtry_default + 2)))

rf_tune_grid <- expand.grid(
  mtry = mtry_values,
  min.node.size = c(1, 5, 10),
  splitrule = "gini"
)

cat("Tuning grid created for Random Forest.\n")

# 5. Train and Evaluate the Model
# Call our reusable function, passing 'ranger'-specific arguments.
set.seed(2025) # for reproducibility
model_rf <- train_and_evaluate_model(
  train_data           = train_rf,
  test_data            = test_rf,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "Random Forest",
  method_caret         = "ranger",
  tuneGrid             = rf_tune_grid,
  importance           = 'impurity' # Specific argument for ranger
)

cat("\nRandom Forest model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates the variable importance plot for the RF model.
plot_rf_visuals <- function(model, top_n = 20) {
  
  # Extract, format, and plot the top N most important variables.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#1f77b4", width = 0.8) + # Blue fill color
    labs(
      title = paste("Top", top_n, "Variables - Random Forest"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)
  
  return(importance_df)
}

importance_df <- plot_rf_visuals(model_rf, top_n = 20)


# 7. Generate All Visuals and Summaries
# Create the specific plots for the Random Forest.
plot_rf_visuals(model_rf)

# Update the overall comparison table and plots with the new model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

```{r}
# Example: dictionary of variable descriptions
var_labels <- c(
  n_children = "Number of children",
  n_children_lt15 = "Number of children under 15",
  n_nuclear = "Number of Spouse + all children (Nuclear family)",
  R2001B_factor.No = "Household does not have Refrigerator",
  n_currently_school = "Number of currently in school (sum(R610 == 2))",
  edu_lowest_group.Tidak.belum.pernah.bersekolah = "Education: never attended school",
  R1817_factor.Firewood = "Main cooking fuel: firewood",
  head_mobile_owner_factor.Head.owns.a.mobile = "Head owns a mobile phone",
  prop_uses_fin_service = "Proportion using financial services",
  R2001H_factor.No = "Household does not have Motorcycle",
  all_adults_mobile_owner_factor.All.adults.own.a.mobile = "All adults own a mobile phone",
  head_fin_service_factor.Head.uses.services = "Head uses financial services",
  R1807_factor.Wood.Planks = "Main wall material: wood planks",
  head_saving_factor.Head.has.savings = "Head has savings",
  n_school = "Number in school (sum(R704 == 2))",
  prop_with_saving = "Proportion with savings",
  R2203_factor.No = "No health insurance",
  prop_adult_smp_plus = "Proportion of adults with ≥ junior high education",
  prop_current_smokers = "Proportion of current smokers",
  avg_age = "Average age of household members"
)

importance_df <- importance_df %>%
  mutate(Description = var_labels[Variable]) %>%
  select(Variable, Description, Importance)

```

```{r}
# Libraries
library(dplyr)
library(ggplot2)
library(stringr)
library(scales)

# --- Build a tidy plotting dataframe (uses Description if available) ---
make_importance_plot_data <- function(importance_df, top_n = 20) {
  importance_df %>%
    mutate(
      # fallback to Variable if Description is missing
      Label = ifelse(!is.null(Description) & !is.na(Description) & Description != "",
                     Description, Variable),
      Importance = as.numeric(Importance)
    ) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n) %>%
    # Wrap long labels for neatness and add ranking numbers
    mutate(
      Label_wrapped = str_wrap(Label, width = 40),
      Label_ranked  = paste0(row_number(), ". ", Label_wrapped)
    )
}

# --- Plot function: clean horizontal bar chart with values ---
plot_importance <- function(importance_df, top_n = 20, title_suffix = "Random Forest") {
  df_plot <- make_importance_plot_data(importance_df, top_n = top_n)

  p <- ggplot(df_plot, aes(x = Importance, y = reorder(Label_ranked, Importance))) +
    geom_col(width = 0.75) +
    geom_text(aes(label = sprintf("%.1f", Importance)),
              hjust = -0.1, size = 3) +
    scale_x_continuous(expand = expansion(mult = c(0, 0.12))) +
    labs(
      title = paste0("Top ", top_n, " Variable Importance — ", title_suffix),
      x = "Importance (scaled)",
      y = NULL,
      caption = "Notes: Importance from caret::varImp (ranger), scaled. Higher indicates greater influence."
    ) +
    theme_minimal(base_size = 11) +
    theme(
      panel.grid.minor = element_blank(),
      panel.grid.major.y = element_blank(),
      axis.text.y = element_text(size = 9),
      plot.title = element_text(face = "bold")
    ) +
    coord_cartesian(clip = "off")  # allow value labels to extend past the axis

  print(p)
  invisible(p)
}

# --- Usage ---
# Ensure `importance_df` has columns: Variable, Description (optional), Importance
plot_importance(importance_df, top_n = 20, title_suffix = "Random Forest")

# Optional: save a high-res figure for your report
ggsave("rf_importance_top20.png", width = 7.5, height = 6.0, dpi = 300)

```

```{r}
# Libraries
library(dplyr)
library(ggplot2)
library(stringr)
library(scales)

# --- Prep: tidy data for plotting (uses Description where available) ---
make_importance_plot_data2 <- function(importance_df, top_n = 20) {
  importance_df %>%
    mutate(
      Label = ifelse(!is.null(Description) & !is.na(Description) & Description != "",
                     Description, Variable),
      Importance = as.numeric(Importance)
    ) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n) %>%
    mutate(
      Label_wrapped = str_wrap(Label, width = 40),
      Label_ranked  = paste0(row_number(), ". ", Label_wrapped)
    )
}

# --- Lollipop plot: academic, crisp, and compact ---
plot_importance_lollipop <- function(importance_df, top_n = 20, title_suffix = "Random Forest") {
  df_plot <- make_importance_plot_data2(importance_df, top_n = top_n)

  ggplot(df_plot, aes(x = Importance, y = reorder(Label_ranked, Importance))) +
    # stems
    geom_segment(aes(x = 0, xend = Importance, yend = reorder(Label_ranked, Importance)),
                 linewidth = 0.5) +
    # dots
    geom_point(size = 2) +
    # value labels at the end of sticks
    geom_text(aes(label = sprintf("%.1f", Importance)),
              hjust = -0.2, size = 3) +
    scale_x_continuous(expand = expansion(mult = c(0, 0.15))) +
    labs(
      title = paste0("Top ", top_n, " Variable Importance — ", title_suffix),
      x = "Importance (scaled)",
      y = NULL,
      caption = "Notes: Importance from caret::varImp (ranger), scaled. Higher indicates greater influence."
    ) +
    theme_classic(base_size = 11) +
    theme(
      axis.text.y = element_text(size = 9),
      plot.title  = element_text(face = "bold"),
      panel.grid  = element_blank()
    ) +
    coord_cartesian(clip = "off")
}

# --- Usage ---
plot_importance_lollipop(importance_df, top_n = 20, title_suffix = "Random Forest")

# Optional: save a high-res figure
ggsave("rf_importance_top20_lollipop.png", width = 7.5, height = 6.0, dpi = 300)
```

```{r}
# Assuming you have the long SHAP data in `shap_long` (with columns `Feature` and `SHAP`)
library(dplyr)
library(tidyr)

shap_side_counts <- shap_long %>%
  # Only keep the top‐15 features you plotted
  filter(Feature %in% top15) %>%
  # Classify each dot as left (neg), right (pos) or zero
  mutate(side = case_when(
    SHAP <  0 ~ "Left (SHAP < 0)",
    SHAP >  0 ~ "Right (SHAP > 0)",
    TRUE      ~ "Zero"
  )) %>%
  # Count per feature × side
  count(Feature, side) %>%
  # Spread into wide format
  pivot_wider(names_from = side, values_from = n, values_fill = 0) %>%
  # (Optionally) add a total column
  mutate(Total = `Left (SHAP < 0)` + `Right (SHAP > 0)` + `Zero`)

print(shap_side_counts)
```

## 4.4 XGBoost

```{r}
# --- Section 4.4: XGBoost Model (Final Version) ---

# 1. Load necessary libraries
# Make sure these are loaded at the start of your session.
library(caret)
library(dplyr)
library(xgboost) # The 'xgboost' engine
library(ggplot2)

cat("\n--- Training XGBoost Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_xgb <- train_smote_universal

nzv_cols <- nearZeroVar(train_xgb, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_xgb <- train_xgb[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_xgb_predictors <- setdiff(names(train_xgb), "MISKIN_KAKO")
test_xgb <- test_data_universal %>%
  select(all_of(final_xgb_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_xgb), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_xgb), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# As you specified, we will tune nrounds, max_depth, and eta.
xgb_tune_grid <- expand.grid(
  nrounds = c(100, 200),
  max_depth = c(3, 6),
  eta = c(0.1, 0.3),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

cat("Tuning grid created for XGBoost.\n")

# 5. Train and Evaluate the Model
# Call our reusable function to train the model and get performance metrics.
set.seed(303) # for reproducibility
model_xgb <- train_and_evaluate_model(
  train_data           = train_xgb,
  test_data            = test_xgb,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "XGBoost",
  method_caret         = "xgbTree",
  tuneGrid             = xgb_tune_grid,
  verbose              = FALSE # Suppress XGBoost's verbose output
)

cat("\nXGBoost model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates the variable importance plot for the XGBoost model.
plot_xgb_visuals <- function(model, top_n = 20) {
  
  # Extract, format, and plot the top N most important variables using caret's method.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#d62728", width = 0.8) + # Red fill color
    labs(
      title = paste("Top", top_n, "Variables - XGBoost"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)
}

# 7. Generate All Visuals and Summaries
# Create the specific plots for XGBoost.
plot_xgb_visuals(model_xgb)

# Update the overall comparison table and plots with the final model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

---
title: "Factor Analysis and Clustering"
author: "Ahmad Risal"
date: "2025-07-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# All Observation

# Section 0: Initial Data Preparation

```{r}
df_model_raw <- readRDS("final_analysis_data.rds")
```

```{r}
# Identify variables with more than one class
multi_class_vars <- sapply(df_model_raw, function(x) length(class(x)) > 1)

# Show variable names and their class attributes
multi_class_details <- sapply(df_model_raw[multi_class_vars], class)

# Display the result
multi_class_details
```

```{r}
# Identify variables with both "ordered" and "factor"
multi_class_vars <- sapply(df_model_raw, function(x) all(c("ordered", "factor") %in% class(x)))

# Forcefully convert all ordered factors to plain factors
df_model_raw[multi_class_vars] <- lapply(
  df_model_raw[multi_class_vars],
  function(x) factor(as.character(x))
)
```

```{r}
# Check updated classes
sapply(df_model_raw[multi_class_vars], class)
```

```{r}
# Identify character columns
char_vars <- sapply(df_model_raw, is.character)

# Convert them to factors
df_model_raw[char_vars] <- lapply(df_model_raw[char_vars], as.factor)
```

```{r}
table(sapply(df_model_raw, class))
```

## Factor Analysis

work on data final_analysis_data

### **Install and Load Required Packages**

```{r}
# install.packages("FactoMineR")
# install.packages("factoextra")
library(FactoMineR)
library(factoextra)
library(dplyr)
```

### **Prepare Data for FAMD**

```{r}
data_susenas <- df_model_raw

table(sapply(data_susenas, function(col) class(col)[1]))


# Get a logical vector: TRUE for character columns, FALSE for others
is_char_col <- sapply(data_susenas, is.character)

# Use the logical vector to subset the names of the dataframe
character_vars <- names(data_susenas)[is_char_col]

# Print the list of character variable names
print(character_vars)
```

```{r}
# Convert any column that is character OR ordered into a factor
data_susenas <- data_susenas %>%
  mutate(across(where(~is.character(.) || is.ordered(.)), as.factor))


```

```{r}
table(sapply(data_susenas, function(col) class(col)[1]))
```

```{r}
data_susenas <- data_susenas %>%
  mutate(across(where(is.ordered), as.factor))
```

```{r}
table(sapply(data_susenas, function(col) class(col)[1]))
```

```{r}
data_famd <- data_susenas %>%
  select(-MISKIN_KAKO)
```

### **Run FAMD**

```{r}
famd_result <- FAMD(data_famd, ncp = 20, graph = FALSE)
```

### **View Explained Variance**

```{r}
# Eigenvalues and % of variance
famd_result$eig
```

### **Scree Plot**

```{r}
fviz_screeplot(famd_result)
```

### **Visualise Variable Contributions**

```{r}
# Contribution to the first two dimensions
fviz_famd_var(famd_result, repel = TRUE)
```

### **Interpret FAMD Results**

```{r}
# Step 1: Eigenvalues and variance explained
famd_result$eig

# Step 2: Variable contributions
famd_result$var$contrib[, 1:3]        # Numeric variables
famd_result$quali.var$contrib[, 1:3]  # Categorical variables
```

### **Export Tables (Optional)**

```{r}
# install.packages("writexl")
library(writexl)

# Extract eigenvalues
eigen_values <- famd_result$eig

# Export to Excel
# write_xlsx(as.data.frame(eigen_values), "famd_eigen_values_3 Dims.xlsx")
```

```{r}
# Extract contribution table
var_contrib <- famd_result$var$contrib[, 1:3]

# Export to Excel
# write_xlsx(as.data.frame(var_contrib), "famd_var_contributions.xlsx")
```

```{r}
# Convert to data frame and include row names as a new column
var_contrib_df <- as.data.frame(var_contrib)
var_contrib_df$Variable <- rownames(var_contrib_df)

# Move 'Variable' column to the front
var_contrib_df <- var_contrib_df[, c("Variable", setdiff(names(var_contrib_df), "Variable"))]

# Export to Excel
# write_xlsx(var_contrib_df, "famd_var_contributions.xlsx")
```

## **Clustering Analysis (input from factor analysis)**

### K-MEANS

#### **Prepare Data for Clustering**

```{r}
# Use first 3 dimensions from FAMD output
clustering_data <- as.data.frame(famd_result$ind$coord[, 1:3])

# Check structure
str(clustering_data)  # Ensure numeric dataframe
```

#### **Load Required Packages**

```{r}
# install.packages(c("factoextra", "cluster"))
library(factoextra)
library(cluster)
```

#### **Step 1: Determine Optimal Number of Clusters (k)**

```{r}
# Elbow Method
#fviz_nbclust(clustering_data, kmeans, method = "wss")

# Silhouette Method
#fviz_nbclust(clustering_data, kmeans, method = "silhouette")
```

#### **Step 2: Apply K-Means Clustering**

```{r}
set.seed(123)  # For reproducibility

# Use factoextra::eclust() for efficient clustering
kmeans_result <- eclust(
  clustering_data, 
  "kmeans", 
  k = 3,          # number of clusters
  nstart = 25,    # repeated initialisation
)
```

#### **Step 3: Visualise Cluster Results**

```{r}
fviz_cluster(
  kmeans_result,
  geom = "point",
  ellipse.type = "norm",
  ggtheme = theme_minimal()
)
```

#### **Step 4: Add Cluster Labels to Dataset**

```{r}
clustering_data$cluster <- kmeans_result$cluster
```

#### **Step 5: Evaluate Clustering Quality**

```{r}
# Silhouette plot
fviz_silhouette(kmeans_result)

# Mean silhouette width
mean(kmeans_result$silinfo$widths[, "sil_width"])

# Interpretation:
# • 0.70–1.00 = Strong structure
# • 0.50–0.70 = Reasonable
# • 0.25–0.50 = Weak
# • < 0.25 = Possibly random

table(kmeans_result$cluster)
```

```{r}
library(ggplot2)

# Create a dataframe of cluster sizes
cluster_sizes <- as.data.frame(table(kmeans_result$cluster))
colnames(cluster_sizes) <- c("Cluster", "Count")

# Plot bar chart with labels
ggplot(cluster_sizes, aes(x = as.factor(Cluster), y = Count, fill = as.factor(Cluster))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Count), vjust = -0.3, size = 4) +  # Add value labels above bars
  labs(
    title = "Cluster Size Distribution (K-Means Clustering)",
    subtitle = paste("Total Observations:", sum(cluster_sizes$Count)),
    x = "Cluster Number",
    y = "Number of Observations",
    fill = "Cluster"
  ) +
  theme_minimal()
```

### GMM

#### Step 1: Prepare Data for Clustering

```{r}
GMM_data <- as.data.frame(famd_result$ind$coord[, 1:4])

# Check structure to ensure it's numeric
str(GMM_data)
```

#### Step 2: Load Required Packages

```{r}
# install.packages("mclust")
# install.packages("factoextra")
# install.packages("cluster")
library(mclust)
library(factoextra)
library(cluster)
```

#### Step 3: Determine Optimal Number of Clusters using BIC

```{r}
gmm_model_selection <- Mclust(GMM_data)
plot(gmm_model_selection, what = "BIC")

# Print model summary to inspect optimal number of components
summary(gmm_model_selection)
```

#### Step 4: Apply GMM Clustering with Optimal G

```{r}
gmm_result <- Mclust(GMM_data, G = gmm_model_selection$G)

# Check number of data points per cluster
table(gmm_result$classification)
```

#### Step 5: Visualise Cluster Results

```{r}
fviz_cluster(
  list(data = GMM_data, cluster = gmm_result$classification),
  geom = "point",
  ellipse.type = "norm",  # Gaussian ellipses
  ggtheme = theme_minimal()
)
```

#### Step 6: Add Cluster Labels to GMM_data

```{r}
GMM_data$cluster <- gmm_result$classification
```

#### Step 7: Evaluate Clustering Quality using Silhouette Score

```{r}
dist_matrix_gmm <- dist(GMM_data[, 1:4])
sil_gmm <- silhouette(GMM_data$cluster, dist_matrix_gmm)

# Silhouette plot
fviz_silhouette(sil_gmm)

# Mean silhouette width
mean(sil_gmm[, "sil_width"])
```

```{r}
library(ggplot2)

# Create a dataframe of GMM cluster sizes
gmm_cluster_sizes <- as.data.frame(table(gmm_result$classification))
colnames(gmm_cluster_sizes) <- c("Cluster", "Count")

# Plot bar chart with labels
ggplot(gmm_cluster_sizes, aes(x = as.factor(Cluster), y = Count, fill = as.factor(Cluster))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Count), vjust = -0.3, size = 4) +
  labs(
    title = "Cluster Size Distribution (Gaussian Mixture Model)",
    subtitle = paste("Total Observations:", sum(gmm_cluster_sizes$Count)),
    x = "Cluster Number",
    y = "Number of Observations",
    fill = "Cluster"
  ) +
  theme_minimal()
```

### CLARA (Scalable PAM for Large Datasets)

#### Step 1: Prepare Data for Clustering

```{r}
CLARA_data <- as.data.frame(famd_result$ind$coord[, 1:4])

# Check structure to ensure numeric dataframe
str(CLARA_data)
```

#### Step 2: Load Required Packages

```{r}
# install.packages(c("cluster", "factoextra"))
library(cluster)
library(factoextra)
```

#### Step 3: Determine Optimal Number of Clusters (k)

```{r}
# Elbow Method
#fviz_nbclust(CLARA_data, kmeans, method = "wss")

# Silhouette Method
#fviz_nbclust(CLARA_data, kmeans, method = "silhouette")
```

#### Step 4: Apply CLARA Clustering

```{r}
set.seed(123)  # For reproducibility

clara_result <- clara(CLARA_data, k = 4, samples = 4)

# View clustering result
table(clara_result$clustering)
```

#### Step 5: Visualise Cluster Results

```{r}
fviz_cluster(
  clara_result,
  geom = "point",
  ellipse.type = "norm",
  ggtheme = theme_minimal()
)
```

#### Step 6: Add Cluster Labels to CLARA_data

```{r}
CLARA_data$cluster <- clara_result$clustering
```

#### Step 7: Evaluate Clustering Quality

```{r}
# Silhouette Plot
fviz_silhouette(clara_result)

# Mean Silhouette Width
mean(clara_result$silinfo$widths[, "sil_width"])
```

#### Step 8: Inspect Cluster Sizes

```{r}
table(CLARA_data$cluster)

library(ggplot2)

# Create a dataframe of CLARA cluster sizes
clara_cluster_sizes <- as.data.frame(table(CLARA_data$cluster))
colnames(clara_cluster_sizes) <- c("Cluster", "Count")

# Plot bar chart with labels
ggplot(clara_cluster_sizes, aes(x = as.factor(Cluster), y = Count, fill = as.factor(Cluster))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Count), vjust = -0.3, size = 4) +
  labs(
    title = "Cluster Size Distribution (CLARA Clustering)",
    subtitle = paste("Total Observations:", sum(clara_cluster_sizes$Count)),
    x = "Cluster Number",
    y = "Number of Observations",
    fill = "Cluster"
  ) +
  theme_minimal()
```

### FUZZY C-MEANS

#### Step 1: Prepare Data for Clustering

```{r}
# Use first 4 dimensions from FAMD output
fuzzy_data <- as.data.frame(famd_result$ind$coord[, 1:4])

# Check structure
str(fuzzy_data)  # Ensure numeric dataframe
```

#### Step 2: Load Required Packages

```{r}
# install.packages("e1071")
# install.packages("factoextra")
library(e1071)
library(factoextra)
```

#### Step 3: Determine Optimal Number of Clusters (k)

```{r}
# Elbow Method
# fviz_nbclust(fuzzy_data, kmeans, method = "wss")

# Silhouette Method
# fviz_nbclust(fuzzy_data, kmeans, method = "silhouette")
```

#### Step 4: Apply Fuzzy C-Means Clustering

```{r}
# Step 4: Apply Fuzzy C-Means Clustering
set.seed(123)

fuzzy_data_scaled <- scale(fuzzy_data)

fcm_result <- cmeans(
  fuzzy_data_scaled,
  centers = 4,
  iter.max = 100,
  m = 2,
  verbose = FALSE
)

# View number of members in each cluster (based on max membership)
table(fcm_result$cluster)
```

#### Step 5: Visualise Cluster Results

```{r}
# Step 5: Visualise Cluster Results
fviz_cluster(
  list(data = fuzzy_data_scaled, cluster = fcm_result$cluster),
  geom = "point",
  ellipse.type = "norm",
  ggtheme = theme_minimal()
)
```

#### Step 6: Add Cluster Labels to fuzzy_data

```{r}
# Step 6: Add Cluster Labels to fuzzy_data
fuzzy_data$cluster <- fcm_result$cluster
```

#### Step 7: Evaluate Clustering Quality

```{r}
# Step 7: Evaluate Clustering Quality
# FCM does not include silhouette output directly.
# So we evaluate using silhouette on hard-cluster assignments.

dist_matrix_fuzzy <- dist(fuzzy_data[, 1:4])
sil_fuzzy <- silhouette(fuzzy_data$cluster, dist_matrix_fuzzy)

# Silhouette plot
fviz_silhouette(sil_fuzzy)

# Mean silhouette width
mean(sil_fuzzy[, "sil_width"])
```

```{r}
library(ggplot2)

# Step 1: Create a frequency table and convert to dataframe
fcm_cluster_sizes <- as.data.frame(table(Cluster = fuzzy_data$cluster))

# Step 2: Plot bar chart
ggplot(fcm_cluster_sizes, aes(x = as.factor(Cluster), y = Freq, fill = as.factor(Cluster))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Freq), vjust = -0.3, size = 4) +
  labs(
    title = "Cluster Size Distribution (Fuzzy C-Means Clustering)",
    subtitle = paste("Total Observations:", sum(fcm_cluster_sizes$Freq)),
    x = "Cluster Number",
    y = "Number of Observations",
    fill = "Cluster"
  ) +
  theme_minimal()
```

## Interpretation of the result

### 1. **Merge Cluster Labels Back to Original Data**

```{r}
library(writexl)
final_clustered_data <- data_susenas

final_clustered_data$cluster <- kmeans_result$cluster

# Export to Excel
write_xlsx(final_clustered_data, "final_clustered_data.xlsx")
```

### **2. Profile Clusters Using Original Variables**

```{r}
library(dplyr)

# Step 1: Define high-contribution variables
important_vars <- c(
  # Socioeconomic Capital
  "highest_cert_group","edu_highest_group","avg_edu_years", "n_sma_or_higher", "edu_head_group", 
  "prop_adult_smp_plus", "n_with_saving", "n_uses_fin_service",
  
  # Household Demography and Dependency
  "n_nuclear", "avg_age", "n_children", "n_currently_school","head_smoker_factor", "head_jobstatus_label",
  
  # Labour Engagement and Lifestyle Exposure
  "avg_working_hours", "common_sector_factor", "head_sector_factor",
  
  # Target Variabel
  "MISKIN_KAKO"
)

# Step 2: Define numeric variables from your previous list
keep_numeric <- c(
  "avg_age","n_nuclear", "n_sma_or_higher", "n_with_saving", "n_uses_fin_service", "n_children", "n_currently_school"
)

# Step 3: Select relevant variables from final_clustered_data
vars_to_profile <- intersect(important_vars, colnames(final_clustered_data))
vars_numeric <- intersect(vars_to_profile, keep_numeric)
vars_categorical <- setdiff(vars_to_profile, vars_numeric)

# Step 4: Summarise numeric variables by cluster
numeric_summary <- final_clustered_data %>%
  group_by(cluster) %>%
  summarise(across(all_of(vars_numeric), ~ round(mean(.x, na.rm = TRUE), 2)))

# Step 5: Summarise categorical variables by mode
get_mode <- function(x) {
  ux <- na.omit(unique(x))
  ux[which.max(tabulate(match(x, ux)))]
}

categorical_mode_summary <- final_clustered_data %>%
  group_by(cluster) %>%
  summarise(across(all_of(vars_categorical), get_mode))

# Step 6: MISKIN_KAKO count and proportion per cluster
miskin_summary <- final_clustered_data %>%
  group_by(cluster) %>%
  summarise(
    miskin_count = sum(MISKIN_KAKO == 1, na.rm = TRUE),
    miskin_proportion = round(mean(MISKIN_KAKO == 1, na.rm = TRUE), 3)
  )

# Step 7: Combine all summaries
cluster_profiles <- numeric_summary %>%
  left_join(categorical_mode_summary, by = "cluster") %>%
  left_join(miskin_summary, by = "cluster")

# View result
cluster_profiles

# Export to Excel
# write_xlsx(cluster_profiles, "cluster_profiles_full.xlsx")
```

### 3. Final Profiling Cluster

```{r}
library(dplyr)

# Step 1: Create summary of key contextual indicators
key_indicators <- final_clustered_data %>%
  group_by(cluster) %>%
  summarise(
    mean_household_size = round(mean(R301, na.rm = TRUE), 2),
    avg_age = round(mean(R407, na.rm = TRUE), 2),
    percent_poor = round(mean(MISKIN_KAKO == 1, na.rm = TRUE), 2),
    .groups = "drop"
  )

# Step 2: Merge with your existing cluster_profiles
cluster_profiles_full <- key_indicators %>%
  left_join(cluster_profiles, by = "cluster")

# View result
cluster_profiles_full

# Load the package
library(writexl)

# Export to Excel
# write_xlsx(cluster_profiles_full, "cluster_profiles_full.xlsx")
```

```{r}
# Frequency table
tab <- table(final_clustered_data$MISKIN_KAKO)

# Percentage
percent <- prop.table(tab) * 100

# Combine into a single table
miskin_table <- data.frame(
  MISKIN_KAKO = names(tab),
  Frequency = as.vector(tab),
  Percentage = round(as.vector(percent), 2)
)

# Show the result
print(miskin_table)
```

### Profiling cluster based on factor analysis result

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

# Step 1: Calculate average score per cluster for each dimension
dimension_summary <- clustering_data %>%
  group_by(cluster) %>%
  summarise(
    Dim1 = mean(Dim.1, na.rm = TRUE),
    Dim2 = mean(Dim.2, na.rm = TRUE),
    Dim3 = mean(Dim.3, na.rm = TRUE),
    .groups = "drop"
  )

# Step 2: Convert to long format for plotting
dimension_long <- dimension_summary %>%
  pivot_longer(cols = starts_with("Dim"), names_to = "Dimension", values_to = "Average")

# Step 3: Plot the dimension profile
ggplot(dimension_long, aes(x = Dimension, y = Average, group = factor(cluster), colour = factor(cluster))) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  labs(
    title = "Cluster Profiles Across FAMD Dimensions",
    x = "FAMD Dimensions",
    y = "Average Score",
    colour = "Cluster"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "top"
  )
```

# Clustering for All Data

### K-MEANS

#### **Step 1: Prepare Data for Clustering**

```{r}
data_clustering_ignorance <- data_susenas %>%
  select(-MISKIN_KAKO)
```

```{r}
# saveRDS(data_clustering_ignorance, file = "data_clustering_ignorance.rds")
```

#### Step 2: Calculate Gower Distance

```{r}
# Load required package
library(cluster)

# Step 2: Compute Gower distance matrix
gower_dist <- daisy(data_clustering_ignorance, metric = "gower")

# Optional: View summary of the distance object
summary(gower_dist)

# Optional: Convert to matrix (if needed later)
# gower_matrix <- as.matrix(gower_dist)
```

#### Step 3: Choose Number of K

```{r}
# Load required packages
library(cluster)
library(factoextra)  # for visualisation

# Step 1: Compute Gower distance (skip if already done)
# gower_dist <- daisy(data_clustering_ignorance, metric = "gower")

# Step 2: Define range of k to evaluate
k_range <- 2:6

# Step 3: Compute average silhouette width for each k
sil_width <- numeric(length(k_range))

for (i in seq_along(k_range)) {
  pam_fit <- pam(gower_dist, diss = TRUE, k = k_range[i])
  sil_width[i] <- pam_fit$silinfo$avg.width
  cat("k =", k_range[i], "| silhouette width =", sil_width[i], "\n")
}

# Step 4: Create a data frame of results
sil_df <- data.frame(k = k_range, silhouette = sil_width)

# Step 5: Plot average silhouette width vs. k
library(ggplot2)
ggplot(sil_df, aes(x = k, y = silhouette)) +
  geom_line(color = "blue") +
  geom_point(size = 2) +
  geom_vline(xintercept = sil_df$k[which.max(sil_df$silhouette)], 
             linetype = "dashed", color = "red") +
  labs(title = "Average Silhouette Width for PAM Clustering",
       subtitle = "Distance: Gower",
       x = "Number of clusters (k)",
       y = "Average silhouette width") +
  theme_minimal()
```

#### Step 4: Apply PAM (Partitioning Around Medoids)

```{r}
# Step 4: Apply PAM clustering
optimal_k <- 2
pam_result <- pam(gower_dist, diss = TRUE, k = optimal_k)

# View summary of the result
summary(pam_result)

# View medoid observations
pam_result$medoids

# Add cluster labels to your original data
data_clustering_ignorance$cluster <- as.factor(pam_result$clustering)
```

```{r}
# Make sure you have the necessary libraries loaded
library(dplyr)
library(ggplot2)

# ---- STEP 1: Count the size of each cluster ----
# We use the 'count()' function from dplyr to get the number of members per cluster.
# This creates a new data frame called 'cluster_sizes' with columns 'cluster' and 'n'.
cluster_sizes <- data_clustering_ignorance %>%
  count(cluster)

# You can print this to see the result:
# print(cluster_sizes)
#  cluster   n
#  <fct>   <int>
#1 1        ...
#2 2        ...
#3 3        ...


# ---- STEP 2: Create the bar plot with labels ----
ggplot(cluster_sizes, aes(x = cluster, y = n, fill = cluster)) +
  
  # Create the bars. geom_col() is used when you have pre-counted y-values.
  geom_col() +
  
  # Add the text labels on top of each bar
  # 'label = n' uses the count column for the label text.
  # 'vjust = -0.5' adjusts the label to sit just above the bar.
  geom_text(aes(label = n), vjust = -0.5, size = 3.5, colour = "black") +
  
  # Use a clean theme
  theme_minimal(base_size = 13) +
  
  # Add titles and labels in UK English
  labs(
    title = "Cluster Size Distribution (K-Medoids)",
    subtitle = "Total Observation: 7530",
    x = "Cluster",
    y = "Number of Members (Count)"
  ) +
  
  # Optional: Use a colour scheme consistent with your previous plot
  scale_fill_brewer(palette = "Dark2") +

  # Tidy up the theme
  theme(
    # The legend is redundant as the x-axis and colours show the same info
    legend.position = "none", 
    panel.grid.major.x = element_blank(), # Remove vertical grid lines
    panel.grid.minor.y = element_blank()  # Remove minor horizontal grid lines
  )
```

```{r}
data_clustering_ignorance$MISKIN_KAKO <- data_susenas$MISKIN_KAKO


#saveRDS(data_clustering_ignorance, file = "result_data_clustering_ignorance.rds")
```

#### Step 5: Analyse the Clusters

#### Descriptive Stat

```{r}
library(dplyr)

data_clustering_ignorance %>%
  group_by(cluster) %>%
  summarise(
    total        = n(),
    miskin_count = sum(MISKIN_KAKO == "MISKIN", na.rm = TRUE),
    pct_miskin   = miskin_count / total * 100
  )

```

```{r}
library(dplyr)

# Step 1: Define high-contribution variables
important_vars <- c(
  # Socioeconomic Capital
  "highest_cert_group","edu_highest_group","avg_edu_years", "n_sma_or_higher", "edu_head_group", 
  "prop_adult_smp_plus", "n_with_saving", "n_uses_fin_service",
  
  # Household Demography and Dependency
  "n_nuclear", "avg_age", "n_children", "n_currently_school","head_smoker_factor", "head_jobstatus_label",
  
  # Labour Engagement and Lifestyle Exposure
  "avg_working_hours", "common_sector_factor", "head_sector_factor",
  
  # Target Variabel
  "MISKIN_KAKO"
)

# Step 2: Define numeric variables from your previous list
keep_numeric <- c(
  "avg_age","n_nuclear", "n_sma_or_higher", "n_with_saving", "n_uses_fin_service", "n_children", "n_currently_school"
)

# Step 3: Select relevant variables from data_clustering_ignorance
vars_to_profile <- intersect(important_vars, colnames(data_clustering_ignorance))
vars_numeric <- intersect(vars_to_profile, keep_numeric)
vars_categorical <- setdiff(vars_to_profile, vars_numeric)

# Step 4: Summarise numeric variables by cluster
numeric_summary <- data_clustering_ignorance %>%
  group_by(cluster) %>%
  summarise(across(all_of(vars_numeric), ~ round(mean(.x, na.rm = TRUE), 2)))

# Step 5: Summarise categorical variables by mode
get_mode <- function(x) {
  ux <- na.omit(unique(x))
  ux[which.max(tabulate(match(x, ux)))]
}

categorical_mode_summary <- data_clustering_ignorance %>%
  group_by(cluster) %>%
  summarise(across(all_of(vars_categorical), get_mode))

# Step 6: MISKIN_KAKO count and proportion per cluster
miskin_summary <- data_clustering_ignorance %>%
  group_by(cluster) %>%
  summarise(
    miskin_count = sum(MISKIN_KAKO == 1, na.rm = TRUE),
    miskin_proportion = round(mean(MISKIN_KAKO == 1, na.rm = TRUE), 3)
  )

# Step 7: Combine all summaries
cluster_profiles <- numeric_summary %>%
  left_join(categorical_mode_summary, by = "cluster") %>%
  left_join(miskin_summary, by = "cluster")

# View result
cluster_profiles

# Export to Excel
library(writexl)
write_xlsx(cluster_profiles, "cluster_profiles_full_ignorance.xlsx")
```

```{r}
# 1. Define a little helper to compute the mode
getmode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# 2. Compute means for numeric vars by cluster
numeric_means <- aggregate(cbind(R407, R301) ~ cluster,
                           data = data_clustering_ignorance,
                           FUN  = function(x) mean(x, na.rm = TRUE))

# 3. Compute mode of the categorical var by cluster
categorical_mode <- with(data_clustering_ignorance,
                         tapply(R404, cluster, getmode))

# 4. Combine into one data.frame
summary_base <- merge(numeric_means,
                      data.frame(cluster = names(categorical_mode),
                                 mode_R404 = as.vector(categorical_mode)),
                      by = "cluster")

summary_base
```

```{r}
# 1. Helper to compute the mode
getmode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# 2. Compute modes per cluster & variable
clusters <- sort(unique(data_clustering_ignorance$cluster))
mode_list <- lapply(clusters, function(cl) {
  sub <- subset(data_clustering_ignorance, cluster == cl)
  data.frame(
    cluster     = cl,
    mode_R607   = getmode(sub$R607),
    mode_R608   = getmode(sub$R608),
    mode_R609   = getmode(sub$R609),
    mode_R614   = getmode(sub$R614),
    stringsAsFactors = FALSE
  )
})

mode_by_cluster <- do.call(rbind, mode_list)
print(mode_by_cluster)
```

```{r}
# 1. Hitung frekuensi
freq_R614 <- table(data_clustering_ignorance$R614)
print(freq_R614)

# 2. Barplot dengan base R
barplot(
  freq_R614,
  main     = "Distribusi R614",
  xlab     = "R614",
  ylab     = "Jumlah",
  col      = c("steelblue","tomato"),
  ylim     = c(0, max(freq_R614) * 1.1)
)
# tambahkan label angka di atas batang
text(
  x      = barplot(freq_R614, plot = FALSE),
  y      = freq_R614,
  label  = freq_R614,
  pos    = 3,    # di atas
  cex    = 0.8
)

# 3. Bar chart dengan ggplot2 (lebih fleksibel)
library(ggplot2)

# ubah ke data frame untuk ggplot
df_R614 <- as.data.frame(freq_R614)
names(df_R614) <- c("R614", "count")

ggplot(df_R614, aes(x = R614, y = count, fill = R614)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = count), vjust = -0.5) +
  labs(
    title = "Distribusi R614",
    x     = "R614",
    y     = "Jumlah"
  ) +
  scale_y_continuous(expand = expansion(mult = c(0, .1)))
```

```{r}
# 2. Compute modes per cluster & variable
clusters <- sort(unique(data_clustering_ignorance$cluster))
mode_list <- lapply(clusters, function(cl) {
  sub <- subset(data_clustering_ignorance, cluster == cl)
  data.frame(
    cluster     = cl,
    mode_R701   = getmode(sub$R701),
    mode_R702   = getmode(sub$R702),
    stringsAsFactors = FALSE
  )
})


mode_by_cluster <- do.call(rbind, mode_list)
print(mode_by_cluster)
```

```{r}
# 1) Mode helper (if not already defined)
getmode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# 2) Identify clusters and the RT-columns
clusters <- sort(unique(data_clustering_ignorance$cluster))
rt_cols  <- paste0("R2001", LETTERS[1:13], "_RT")   # A–M

# 3) Build mode list
mode_list <- lapply(clusters, function(cl) {
  sub <- subset(data_clustering_ignorance, cluster == cl)
  
  # modes for R801 & R802 plus all the R2001*_RT columns
  all_cols <- c("R801", "R802", rt_cols)
  modes    <- sapply(all_cols, function(col) getmode(sub[[col]]), USE.NAMES = FALSE)
  
  # name them mode_<col>
  names(modes) <- paste0("mode_", all_cols)
  
  # assemble
  data.frame(
    cluster = cl,
    as.list(modes),
    stringsAsFactors = FALSE
  )
})

# 4) Combine into one data.frame
mode_by_cluster <- do.call(rbind, mode_list)
rownames(mode_by_cluster) <- NULL

print(mode_by_cluster)
```

```{r}
# 1) Helper to compute the mode
getmode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# 2) Define your feature list
features <- c(
  "R1802_RT", "R1804_RT", "R1805_RT", "R1807_RT", 
  "R1808_RT", "R1812_RT", "R1816_RT", "R1817_RT", 
  "R2202_RT", "R2203_RT", "R1806A_RT", "R1809A_RT", 
  "R1810A_RT", "R1811A_RT", "R1813A_RT", "R1813B_RT", 
  "R1813C_RT", "R1813D_RT", "R1813E_RT", "R1814A_RT", 
  "R1815A_RT"
)

# 3) Split into numeric vs. categorical
num_feat <- "R1804_RT"
cat_feats <- setdiff(features, num_feat)

# 4) Tidyverse approach
library(dplyr)

mode_mean_by_cluster <- data_clustering_ignorance %>%
  group_by(cluster) %>%
  summarise(
    # mean for the numeric feature
    mean_R1804_RT = mean(R1804_RT, na.rm = TRUE),
    # modes for all the other RT features
    across(
      all_of(cat_feats),
      ~ getmode(.x),
      .names = "mode_{.col}"
    ),
    .groups = "drop"
  )

print(mode_mean_by_cluster)
```

```{r}
library(dplyr)

# 1) Mode helper
getmode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# 2) Summarise by cluster
summary_by_cluster <- data_clustering_ignorance %>%
  group_by(cluster) %>%
  summarise(
    mode_R705 = getmode(R705),
    mode_R706 = getmode(R706),
    mode_R707 = getmode(R707),
    mean_R708 = mean(R708, na.rm = TRUE),
    mean_R709 = mean(R709, na.rm = TRUE),
    .groups = "drop"
  )

print(summary_by_cluster)
```

```{r}
library(ggplot2)

hist(
  data_clustering_ignorance$R407,
  breaks = 60,                        # adjust number of bins as needed
  main   = "Distribution of Age",
  xlab   = "R407",
  ylab   = "Frequency",
  col    = "lightblue",
  border = "white"
)
```

```{r}
# 1. Compute counts and percentages
freq <- table(data_clustering_ignorance$MISKIN_KAKO)
pct  <- prop.table(freq) * 100

# 2. Assemble into a data frame
dist_MISKIN_KAKO <- data.frame(
  MISKIN_KAKO = names(freq),
  Count       = as.integer(freq),
  Percent     = round(as.numeric(pct), 2),
  row.names   = NULL,
  stringsAsFactors = FALSE
)

# 3. View
print(dist_MISKIN_KAKO)
```

```{r}
library(dplyr)
library(ggplot2)
library(scales)  # for percent_format()

data_clustering_ignorance %>%
  count(MISKIN_KAKO) %>%                            # get counts
  mutate(
    pct = n / sum(n),                               # compute proportions
    pct_label = percent(pct, accuracy = 0.1)        # format as "xx.x%"
  ) %>%
  ggplot(aes(x = MISKIN_KAKO, y = n, fill = MISKIN_KAKO)) +
    geom_col(width = 0.6, show.legend = FALSE) +
    geom_text(aes(label = paste0(n, "\n", pct_label)),
              vjust = -0.5, size = 4) +              # two‐line labels: count + percent
    labs(
      title = "Distribusi MISKIN_KAKO",
      x     = NULL,
      y     = "Jumlah"
    ) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
    theme_minimal(base_size = 14)
```

#### MDS Plot of Cluster

```{r}
# # Perform classical multidimensional scaling
# mds <- cmdscale(gower_dist, k = 2)
# 
# mds_df <- data.frame(
#   Dim1 = mds[, 1],
#   Dim2 = mds[, 2],
#   Cluster = data_clustering_ignorance$cluster
# )
# 
# # Plot using ggplot2
# library(ggplot2)
# ggplot(mds_df, aes(x = Dim1, y = Dim2, color = Cluster)) +
#   geom_point(alpha = 0.6, size = 1.5) +
#   labs(title = "MDS Plot of PAM Clusters (Gower Distance)") +
#   theme_minimal()
```

#### Try stratified sampling for plotting

```{r}
set.seed(42)

library(dplyr)

# Sample 3000 rows stratified by cluster
subsample_df <- data_clustering_ignorance %>%
  group_by(cluster) %>%
  sample_frac(size = 3000 / nrow(data_clustering_ignorance)) %>%
  ungroup()

# Extract Gower matrix rows/columns for these sampled rows
sample_ids <- as.integer(rownames(subsample_df))
gower_sub <- as.dist(as.matrix(gower_dist)[sample_ids, sample_ids])

# MDS
mds <- cmdscale(gower_sub, k = 2)
mds_df <- data.frame(Dim1 = mds[,1], Dim2 = mds[,2], Cluster = subsample_df$cluster)

# Plot
library(ggplot2)
ggplot(mds_df, aes(x = Dim1, y = Dim2, color = Cluster)) +
  geom_point(alpha = 0.6) +
  theme_minimal() +
  labs(title = "MDS Plot (Stratified Subset) of Clusters")
```

```{r}
library(ggplot2)

ggplot(mds_df, aes(x = Dim1, y = Dim2, color = Cluster, fill = Cluster)) +
  # Draw cluster ellipses
  stat_ellipse(geom = "polygon", alpha = 0.15, colour = NA) +
  
  # Plot individual points
  geom_point(alpha = 0.6, size = 1.8, shape = 21, stroke = 0.3) +
  
  # Clean and minimalist theme
  theme_minimal(base_size = 13) +
  
  # Axis labels and title
  labs(
    title = "MDS Plot (Stratified Subset) of Clusters",
    x = "Dimension 1",
    y = "Dimension 2",
    color = "Cluster",
    fill = "Cluster"
  ) +
  
  # Better color palette (optional)
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Pastel2") +
  
  # Improve legend and spacing
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey85")
  )
```

```{r}
# Pastikan library ggplot2 sudah dimuat
library(ggplot2)

# Asumsikan 'mds_df' adalah data frame Anda dengan kolom Dim1, Dim2, dan Cluster

ggplot(mds_df, aes(x = Dim1, y = Dim2, color = Cluster, fill = Cluster)) +
  
  # Gambar elips cluster dengan garis tepi yang jelas
  # Perubahan utama ada di baris ini:
  # - Menghapus 'colour = NA'
  # - Menambahkan 'size = 1' untuk membuat garis lebih tebal
  # - Sedikit menaikkan alpha untuk isian agar lebih terlihat
  stat_ellipse(geom = "polygon", alpha = 0.2, size = 1) +
  
  # Plot titik data individual
  geom_point(alpha = 0.7, size = 1.8, shape = 21, stroke = 0.3) +
  
  # Tema minimalis yang bersih
  theme_minimal(base_size = 13) +
  
  # Judul dan label sumbu
  labs(
    title = "MDS Plot with All Variable",
    subtitle = "Clustering with All Variable",
    x = "Dimension 1",
    y = "Dimension 2",
    color = "Cluster",
    fill = "Cluster"
  ) +
  
  # Palet warna kontras yang sudah bagus
  # Dark2 untuk garis tepi (points dan ellipse outline)
  # Pastel2 untuk isian (ellipse fill)
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Pastel2") +
  
  # Memperbaiki legenda dan layout
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey90", linetype = "dashed"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "grey40")
  )
```

#### Step 6: Validate the Clustering

#### Step 7: Interpret and Label the Clusters

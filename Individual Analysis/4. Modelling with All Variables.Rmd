---
title: "Predicting Ignorance New"
author: "Ahmad Risal"
date: "2025-06-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 0: Initial Data Preparation

```{r}
df_model_raw <- readRDS("data_susenas.rds")
```

```{r}
# Identify variables with more than one class
multi_class_vars <- sapply(df_model_raw, function(x) length(class(x)) > 1)

# Show variable names and their class attributes
multi_class_details <- sapply(df_model_raw[multi_class_vars], class)

# Display the result
multi_class_details
```

```{r}
# Identify variables with both "ordered" and "factor"
multi_class_vars <- sapply(df_model_raw, function(x) all(c("ordered", "factor") %in% class(x)))

# Forcefully convert all ordered factors to plain factors
df_model_raw[multi_class_vars] <- lapply(
  df_model_raw[multi_class_vars],
  function(x) factor(as.character(x))
)
```

```{r}
# Check updated classes
sapply(df_model_raw[multi_class_vars], class)
```

```{r}
# Identify character columns
char_vars <- sapply(df_model_raw, is.character)

# Convert them to factors
df_model_raw[char_vars] <- lapply(df_model_raw[char_vars], as.factor)
```

```{r}
table(sapply(df_model_raw, class))
```

```{r}
df_model_raw <- df_model_raw[, !names(df_model_raw) %in% "R102"]

df_model_raw$MISKIN_KAKO <- ifelse(df_model_raw$MISKIN_KAKO == "MISKIN", 1, 0)
df_model_raw$MISKIN_KAKO <- as.numeric(df_model_raw$MISKIN_KAKO)
```

```{r}
# Load required libraries
library(dplyr)
library(caret)

# 2. Standardise column names
names(df_model_raw) <- make.names(names(df_model_raw), unique = TRUE)
cat("Column names standardised.\n")

# 3. Clean factor levels by replacing spaces with dots
df_model_raw <- df_model_raw %>%
  mutate(across(where(is.factor), ~ {
    levels(.) <- make.names(levels(.), unique = FALSE)
    .
  }))
cat("Factor levels cleaned using make.names().\n")

# 4. Ensure MISKIN_KAKO is a factor with correct labels
# Level order: 0 = TIDAK.MISKIN (reference), 1 = MISKIN (positive)
df_model_raw <- df_model_raw %>%
  mutate(MISKIN_KAKO = factor(MISKIN_KAKO, levels = c(0, 1), labels = c("TIDAK.MISKIN", "MISKIN")))
cat("MISKIN_KAKO formatted as factor with labels.\n")

# 5. Check for missing values
na_summary <- sapply(df_model_raw, function(x) sum(is.na(x)))
na_summary_display <- na_summary[na_summary > 0]
if(length(na_summary_display) > 0) {
  cat("\nVariables with missing values:\n")
  print(na_summary_display)
} else {
  cat("\nNo missing values found.\n")
}

# 6. Create dummy variables
categorical_features <- names(df_model_raw)[sapply(df_model_raw, is.factor)]
categorical_features <- setdiff(categorical_features, "MISKIN_KAKO") # Exclude target

if (length(categorical_features) > 0) {
  # Create dummy variables, dropping one level per factor to avoid multicollinearity
  # --- THIS IS THE MODIFIED LINE ---
  dummy_model <- dummyVars(~ ., data = df_model_raw[, categorical_features], fullRank = TRUE)
  # ---------------------------------
  
  encoded_features <- predict(dummy_model, newdata = df_model_raw[, categorical_features])
  
  # Convert to data.frame to ensure compatibility
  encoded_features <- as.data.frame(encoded_features)

  # Bind encoded features with remaining (non-categorical + target)
  data_ml <- df_model_raw %>%
    select(-one_of(categorical_features)) %>% # remove original factor columns
    bind_cols(encoded_features)
} else {
  data_ml <- df_model_raw
}

# 7. Final structure and summary
cat("\nStructure of final modelling dataset (data_ml):\n")
print(str(data_ml))
cat("\nPreview column names:\n")
print(head(names(data_ml)))
cat("\nClass balance:\n")
print(table(data_ml$MISKIN_KAKO))

```

# Section 1: Universal Train and Test Split

```{r}
library(caret)
library(dplyr)

# 1. Convert MISKIN_KAKO to numeric (0 = TIDAK.MISKIN, 1 = MISKIN)
data_ml <- data_ml %>%
  mutate(MISKIN_KAKO = as.integer(MISKIN_KAKO == "MISKIN"))
cat("MISKIN_KAKO converted to numeric (0 = TIDAK.MISKIN, 1 = MISKIN).\n")

# 2. Set random seed for reproducibility
set.seed(123)

# 3. Stratified sampling (75% train, 25% test)
train_index_universal <- createDataPartition(
  y = data_ml$MISKIN_KAKO,
  p = 0.75,
  list = FALSE
)

# 4. Split into training and testing datasets
train_data_universal <- data_ml[train_index_universal, ]
test_data_universal  <- data_ml[-train_index_universal, ]

# 5. Summary of split
cat("\nDimensions of train_data_universal:\n")
print(dim(train_data_universal))
cat("Class balance in train_data_universal:\n")
print(prop.table(table(train_data_universal$MISKIN_KAKO)))

cat("\nDimensions of test_data_universal:\n")
print(dim(test_data_universal))
cat("Class balance in test_data_universal:\n")
print(prop.table(table(test_data_universal$MISKIN_KAKO)))
```

# **Section 2: Apply SMOTE on Universal Training Data**

```{r}
# library(smotefamily)
# library(dplyr)
# library(ggplot2)
# 
# cat("Step 2: Applying SMOTE on the universal training data...\n")
# 
# # --- 2.1 Prepare data for SMOTE ---
# # Separate features and target
# predictor_cols_for_smote <- setdiff(names(train_data_universal), "MISKIN_KAKO")
# X_train_universal <- train_data_universal[, predictor_cols_for_smote, drop = FALSE]
# y_train_universal <- train_data_universal$MISKIN_KAKO  # numeric (0 = TIDAK.MISKIN, 1 = MISKIN)
# 
# # Show class distribution BEFORE SMOTE
# cat("\nClass distribution BEFORE SMOTE:\n")
# initial_table <- table(y_train_universal)
# print(initial_table)
# 
# # Visualise before SMOTE
# df_before <- data.frame(
#   Class = factor(ifelse(y_train_universal == 1, "MISKIN", "TIDAK.MISKIN"), levels = c("TIDAK.MISKIN", "MISKIN"))
# )
# 
# ggplot(df_before, aes(x = Class, fill = Class)) +
#   geom_bar() +
#   labs(title = "Class Distribution BEFORE SMOTE", y = "Count") +
#   theme_minimal()
# 
# # --- 2.2 Calculate dup_size for ~50:50 balance ---
# minority <- min(initial_table)
# majority <- max(initial_table)
# dup_size <- floor((majority / minority) - 1)
# dup_size <- max(dup_size, 1)  # Ensure at least 1
# 
# # --- 2.3 Apply SMOTE ---
# cat(paste0("Applying SMOTE with K = 5 and dup_size = ", dup_size, "...\n"))
# smote_result <- SMOTE(X_train_universal, y_train_universal, K = 5, dup_size = dup_size)
# 
# train_smote_universal <- smote_result$data
# names(train_smote_universal)[ncol(train_smote_universal)] <- "MISKIN_KAKO"  # Rename last column
# 
# # Ensure target is numeric 0/1
# train_smote_universal$MISKIN_KAKO <- as.integer(train_smote_universal$MISKIN_KAKO)
# 
# # --- 2.4 Visualise BEFORE and AFTER in the same chart ---
# 
# # Combine data into one long-format dataframe for plotting
# df_before <- data.frame(
#   Class = factor(ifelse(y_train_universal == 1, "MISKIN", "TIDAK.MISKIN"), levels = c("TIDAK.MISKIN", "MISKIN")),
#   Source = "Before SMOTE"
# )
# 
# df_after <- data.frame(
#   Class = factor(ifelse(train_smote_universal$MISKIN_KAKO == 1, "MISKIN", "TIDAK.MISKIN"), levels = c("TIDAK.MISKIN", "MISKIN")),
#   Source = "After SMOTE"
# )
# 
# df_combined <- bind_rows(df_before, df_after)
# 
# # Summarise counts for label placement
# label_df <- df_combined %>%
#   group_by(Source, Class) %>%
#   summarise(Count = n(), .groups = "drop")
# 
# # Plot
# ggplot(label_df, aes(x = Class, y = Count, fill = Source)) +
#   geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
#   geom_text(aes(label = Count), 
#             position = position_dodge(width = 0.8), 
#             vjust = -0.2, size = 4) +
#   labs(title = "Class Distribution Before and After SMOTE",
#        x = "Class",
#        y = "Count") +
#   scale_fill_manual(values = c("Before SMOTE" = "#1f77b4", "After SMOTE" = "#ff7f0e")) +
#   theme_minimal(base_size = 12)
# 
# # --- 2.5 Check for missing values ---
# cat("\nChecking for NA values in SMOTE dataset...\n")
# na_summary_smote <- sapply(train_smote_universal, function(x) sum(is.na(x)))
# print(na_summary_smote[na_summary_smote > 0])
```

```{r}
# Load required libraries
library(dplyr)
library(themis)

cat("Step 2: Applying SMOTE on the universal training data for a 60:40 balance...\n")

# --- 2.1 Show class distribution BEFORE SMOTE ---
cat("\nClass distribution BEFORE SMOTE:\n")
print(table(train_data_universal$MISKIN_KAKO))
cat("\nProportions BEFORE SMOTE:\n")
print(prop.table(table(train_data_universal$MISKIN_KAKO)))

# --- 2.2 Apply SMOTE using themis ---
# Themis needs the target variable to be a factor
train_data_for_smote <- train_data_universal %>%
  mutate(MISKIN_KAKO = factor(MISKIN_KAKO, levels = c(0, 1), labels = c("TIDAK.MISKIN", "MISKIN")))

# Target ratio: 40% minority (MISKIN) and 60% majority (TIDAK.MISKIN)
target_ratio <- 40 / 60 

# Apply SMOTE, explicitly calling the function from the themis package
set.seed(123)
train_smote_universal <- themis::smote(
  train_data_for_smote, 
  var = "MISKIN_KAKO",
  over_ratio = target_ratio
)

# Convert target variable back to numeric (0/1) for modelling
train_smote_universal <- train_smote_universal %>%
  mutate(MISKIN_KAKO = as.integer(MISKIN_KAKO == "MISKIN"))
  
# --- 2.3 Show class distribution AFTER SMOTE ---
cat("\nClass distribution AFTER SMOTE:\n")
print(table(train_smote_universal$MISKIN_KAKO))
cat("\nProportions AFTER SMOTE:\n")
print(prop.table(table(train_smote_universal$MISKIN_KAKO)))

cat("\nSMOTE application complete. The dataset 'train_smote_universal' is ready.\n")
```

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# 1. Prepare data for plotting
# Create a dataframe for the original data
df_before <- data.frame(
  MISKIN_KAKO = train_data_universal$MISKIN_KAKO, # Data before SMOTE
  Source = "Before SMOTE"
)

# Create a dataframe for the balanced data
df_after <- data.frame(
  MISKIN_KAKO = train_smote_universal$MISKIN_KAKO, # Data after SMOTE
  Source = "After SMOTE"
)

# 2. Combine the two dataframes
df_combined <- bind_rows(df_before, df_after)

# 3. Convert the target variable to a descriptive factor for better labels
df_combined <- df_combined %>%
  mutate(MISKIN_KAKO_label = factor(MISKIN_KAKO,
                             levels = c(0, 1),
                             labels = c("Not Poor", "Poor")))

# 4. Create the bar chart
ggplot(df_combined, aes(x = MISKIN_KAKO_label, fill = Source)) +
  geom_bar(position = "dodge") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)), # Automatically adds the count as a label
    position = position_dodge(width = 0.9),
    vjust = -0.5, # Adjust vertical position to be above the bar
    size = 4
  ) +
  scale_fill_manual(values = c("Before SMOTE" = "#1f77b4", "After SMOTE" = "#ff7f0e")) +
  labs(
    title = "Class Distribution Before and After SMOTE",
    x = "Poverty Status (MISKIN_KAKO)",
    y = "Number of Samples",
    fill = "Dataset"
  ) +
  theme_minimal(base_size = 14)
```

# Section 4: Modelling

## 4.1: Reusable Functions

```{r}
# ===================================================================
# Final Reusable Functions for Modelling & Evaluation
# ===================================================================

# 1. Load All Required Libraries
# -------------------------------------------------------------------
library(caret)
library(pROC)
library(dplyr)
library(tidyr)
library(ggplot2)
library(purrr)
library(tibble)
library(readr)
library(writexl)
library(doParallel)


# 2. Setup Parallel Processing & Global Result Lists
# -------------------------------------------------------------------
# Use n-1 cores to leave one free for system processes
num_cores <- detectCores() - 1
if (num_cores < 1) num_cores <- 1
registerDoParallel(cores = num_cores)
cat(paste0("Registered ", num_cores, " cores for parallel processing.\n"))

# Initialise global lists to store results from all models
all_model_performance_metrics <- list()
all_cv_accuracy_results <- list()
all_roc_data <- list()
all_model_parameters <- list()

cat("Global result lists created.\n")


# 3. Reusable Function Definitions
# -------------------------------------------------------------------

# --- Function 1: Train and Evaluate a Model ---
train_and_evaluate_model <- function(
  train_data,
  test_data,
  target_variable_name,
  model_name,
  method_caret,
  model_formula   = NULL,
  tuneGrid        = NULL,
  preProc_methods = NULL,
  ...
) {
  cat(paste0("\n--- Training and Evaluating ", model_name, " Model ---\n"))

  # Control for 5-fold CV optimising ROC
  fitControl <- trainControl(
    method            = "cv",
    number            = 5,
    savePredictions   = "final",
    classProbs        = TRUE,
    summaryFunction   = twoClassSummary,
    allowParallel     = TRUE
  )

  # Convert numeric binary target to factor for classification
  train_data[[target_variable_name]] <- factor(
    ifelse(train_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
    levels = c("NotPoor", "Poor")
  )
  test_data[[target_variable_name]] <- factor(
    ifelse(test_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
    levels = c("NotPoor", "Poor")
  )

  # Build formula
  formula_obj <- if (is.null(model_formula)) 
    as.formula(paste0(target_variable_name, " ~ .")) 
  else model_formula

  # Set seed and train
  set.seed(456)
  trained_model <- train(
    formula_obj,
    data      = train_data,
    method    = method_caret,
    trControl = fitControl,
    tuneGrid  = tuneGrid,
    preProcess= preProc_methods,
    metric    = "ROC",
    ...
  )
  cat(paste0(model_name, " model training completed.\n"))

  # Extract CV accuracy for diagnostic plot
  preds <- trained_model$pred
  if (!is.null(trained_model$bestTune)) {
    for (param in names(trained_model$bestTune)) {
      preds <- preds[preds[[param]] == trained_model$bestTune[[param]], ]
    }
  }
  cv_results_df <- preds %>%
    group_by(Resample) %>%
    summarize(Accuracy = mean(pred == obs, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(Model = model_name)
  all_cv_accuracy_results[[model_name]] <<- cv_results_df

  # Test set evaluation with optimal threshold for F1-Score
  test_probs <- predict(trained_model, test_data, type = "prob")[, "Poor"]
  actuals <- test_data[[target_variable_name]]
  roc_obj <- roc(response = actuals, predictor = test_probs, levels = c("NotPoor", "Poor"))
                   
  # Find optimal threshold that maximizes F1-Score
  coords_df <- coords(roc_obj, "all", ret = c("threshold", "precision", "recall")) %>%
    as.data.frame() %>%
    mutate(f1 = 2 * (precision * recall) / (precision + recall)) %>%
    filter(is.finite(f1))

  optimal_threshold <- coords_df %>%
    slice_max(order_by = f1, n = 1) %>%
    pull(threshold)

  cat(paste0("Optimal threshold found to maximise F1-Score: ", round(optimal_threshold, 4), "\n"))

  # Apply the optimal threshold for predictions
  test_preds <- factor(ifelse(test_probs >= optimal_threshold, "Poor", "NotPoor"),
                       levels = c("NotPoor", "Poor"))
  
  cm <- confusionMatrix(test_preds, actuals, positive = "Poor")

  # Store ROC curve data
  all_roc_data[[model_name]] <<- data.frame(
    FPR   = 1 - roc_obj$specificities,
    TPR   = roc_obj$sensitivities,
    Model = model_name
  )

  # Store final performance metrics
  all_model_performance_metrics[[model_name]] <<- list(
    Accuracy        = cm$overall["Accuracy"],
    Sensitivity     = cm$byClass["Sensitivity"], # Recall
    Specificity     = cm$byClass["Specificity"],
    Precision       = cm$byClass["Precision"],
    F1_Score        = cm$byClass["F1"],
    AUC             = as.numeric(auc(roc_obj)),
    Best_Threshold  = optimal_threshold
  )

  # Store tuning parameters
  if (!is.null(trained_model$bestTune)) {
    param_str <- paste(names(trained_model$bestTune), trained_model$bestTune, sep = "=", collapse = ", ")
  } else {
    param_str <- "—"
  }
  all_model_parameters[[model_name]] <<- param_str

  cat(paste0("Evaluation for ", model_name, " on test data completed.\n"))
  return(trained_model)
}


# --- Function 2: Create a Summary Comparison Table ---
create_comparison_table <- function() {
  if (length(all_model_performance_metrics) == 0) {
    cat("No model performance metrics available.\n")
    return(NULL)
  }
  
  # Safely bind model names and metrics into a data frame
  df_summary <- lapply(names(all_model_performance_metrics), function(model_name) {
    metrics <- all_model_performance_metrics[[model_name]]
    params  <- all_model_parameters[[model_name]]
    
    data.frame(
      Model           = model_name,
      Accuracy        = round(metrics$Accuracy, 4),
      F1_Score        = round(metrics$F1_Score, 4),
      AUC             = round(metrics$AUC, 4),
      Sensitivity     = round(metrics$Sensitivity, 4),
      Specificity     = round(metrics$Specificity, 4),
      Precision       = round(metrics$Precision, 4),
      Best_Threshold  = round(metrics$Best_Threshold, 4),
      Parameters      = paste(params, collapse = ", ")
    )
  }) %>%
    bind_rows()

  return(df_summary)
}


# --- Function 3: Plot Cross-Validation Accuracy Boxplot (Diagnostic) ---
plot_cv_accuracy_boxplot <- function() {
  if (length(all_cv_accuracy_results) == 0) return()
  df <- bind_rows(all_cv_accuracy_results) %>% filter(is.finite(Accuracy))
  p <- ggplot(df, aes(x = reorder(Model, Accuracy, median), y = Accuracy, fill = Model)) +
    geom_boxplot() +
    labs(
      title = "Model Stability: Cross-Validation Accuracy",
      subtitle = "Shows performance distribution during training on SMOTE data",
      x = "Model", y = "Accuracy"
    ) +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = 25, hjust = 1))
  print(p)
}


# --- Function 4: Plot Final Performance Bar Chart ---
plot_all_performance_bar_chart <- function(df, metric_to_plot = "F1_Score") {
  if (!metric_to_plot %in% names(df)) stop("Metric not found: ", metric_to_plot)
  
  p <- ggplot(df, aes_string(x = sprintf("reorder(Model, -%s)", metric_to_plot), y = metric_to_plot, fill = "Model")) +
    geom_col(width = 0.7) +
    geom_text(aes_string(label = metric_to_plot), vjust = -0.5, size = 3.5) +
    labs(
      title = paste("Final Model Performance on Test Set"),
      subtitle = paste("Metric:", metric_to_plot),
      x = "Model", y = metric_to_plot
    ) +
    theme_minimal() +
    theme(legend.position = "none", axis.text.x = element_text(angle = 25, hjust = 1))
  print(p)
}


# --- Function 5: Plot All ROC Curves ---
plot_all_roc_curves <- function() {
  if (length(all_roc_data) == 0) return()
  df <- bind_rows(all_roc_data)
  
  # Add AUC values for the legend
  auc_labels <- create_comparison_table() %>%
    select(Model, AUC) %>%
    mutate(label = paste0(Model, " (AUC = ", AUC, ")"))
    
  df <- df %>% left_join(auc_labels, by = "Model")

  p <- ggplot(df, aes(x = FPR, y = TPR, colour = label)) +
    geom_line(linewidth = 1.1) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "grey") +
    labs(
      title = "ROC Curves for All Models on Test Set",
      x = "False Positive Rate (1 - Specificity)", 
      y = "True Positive Rate (Sensitivity)",
      colour = "Model"
    ) +
    theme_minimal()
  print(p)
}


# --- Function 6: Export Summary Table to Excel ---
export_summary_to_excel <- function(summary_df, file_path = "model_performance_summary.xlsx") {
  if (is.null(summary_df) || nrow(summary_df) == 0) {
    cat("Summary data frame is empty. Nothing to export.\n")
    return()
  }
  
  write_xlsx(summary_df, path = file_path)
  cat(paste0("\nFinal model performance summary exported to: ", file_path, "\n"))
}

```

```{r}
# Global lists to store results
all_model_performance_metrics <- list()
all_cv_accuracy_results <- list()
all_roc_data <- list()
all_model_parameters <- list()

cat("Global result lists created.\n")
```

### 4.1.2 Reusable function maximise sensitivity

```{r}
# # -------------------------------------------------------------------
# # Initialise global lists for sensitivity-focused evaluation
# # -------------------------------------------------------------------
# all_model_performance_metrics_sens <- list()
# all_cv_accuracy_results_sens     <- list()
# all_roc_data_sens                <- list()
# all_model_parameters_sens        <- list()
# 
# cat("Global result lists for sensitivity-focus created.\n")
# 
# 
# # -------------------------------------------------------------------
# # Function: Train & evaluate model, picking threshold to maximise Sensitivity
# # -------------------------------------------------------------------
# train_and_evaluate_model_sens <- function(
#   train_data,
#   test_data,
#   target_variable_name,
#   model_name,
#   method_caret,
#   model_formula   = NULL,
#   tuneGrid        = NULL,
#   preProc_methods = NULL,
#   min_specificity = 0.4,
#   ...
# ) {
#   cat(paste0("\n--- Training and Evaluating ", model_name,
#              " (sensitivity-focus) Model ---\n"))
# 
#   # same CV control as before
#   fitControl <- trainControl(
#     method            = "cv",
#     number            = 5,
#     savePredictions   = "final",
#     classProbs        = TRUE,
#     summaryFunction   = twoClassSummary,
#     allowParallel     = TRUE
#   )
# 
#   # factorise target
#   train_data[[target_variable_name]] <- factor(
#     ifelse(train_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
#     levels = c("NotPoor", "Poor")
#   )
#   test_data[[target_variable_name]] <- factor(
#     ifelse(test_data[[target_variable_name]] == 1, "Poor", "NotPoor"),
#     levels = c("NotPoor", "Poor")
#   )
# 
#   # build formula
#   formula_obj <- if (is.null(model_formula)) 
#     as.formula(paste0(target_variable_name, " ~ .")) 
#   else model_formula
# 
#   set.seed(456)
#   trained_model <- train(
#     formula_obj,
#     data      = train_data,
#     method    = method_caret,
#     trControl = fitControl,
#     tuneGrid  = tuneGrid,
#     preProcess= preProc_methods,
#     metric    = "ROC",
#     ...
#   )
#   cat(paste0(model_name, " training completed.\n"))
# 
#   # store CV accuracy
#   preds <- trained_model$pred
#   if (!is.null(trained_model$bestTune)) {
#     for (param in names(trained_model$bestTune)) {
#       preds <- preds[preds[[param]] == trained_model$bestTune[[param]], ]
#     }
#   }
#   cv_df <- preds %>%
#     group_by(Resample) %>%
#     summarize(Accuracy = mean(pred == obs, na.rm = TRUE)) %>%
#     ungroup() %>%
#     mutate(Model = model_name)
#   all_cv_accuracy_results_sens[[model_name]] <<- cv_df
# 
#   # ROC & threshold search
#   test_probs <- predict(trained_model, test_data, type = "prob")[, "Poor"]
#   actuals    <- test_data[[target_variable_name]]
#   roc_obj    <- roc(response = actuals, predictor = test_probs,
#                     levels = c("NotPoor", "Poor"))
# 
#   coords_df <- coords(
#     roc_obj, "all",
#     ret = c("threshold", "sensitivity", "specificity")
#   ) %>% as.data.frame()
# 
#   # impose min specificity, pick highest sens, tie-break on specificity
#   optimal_threshold_sens <- coords_df %>%
#     filter(specificity >= min_specificity) %>%
#     slice_max(order_by = sensitivity, n = 1, with_ties = TRUE) %>%
#     slice_max(order_by = specificity, n = 1) %>%
#     pull(threshold)
# 
#   cat(paste0("Threshold for max sensitivity (spec ≥ ", min_specificity, 
#              "): ", round(optimal_threshold_sens, 4), "\n"))
# 
#   # apply threshold
#   test_preds <- factor(
#     ifelse(test_probs >= optimal_threshold_sens, "Poor", "NotPoor"),
#     levels = c("NotPoor", "Poor")
#   )
#   cm <- confusionMatrix(test_preds, actuals, positive = "Poor")
# 
#   # store ROC data
#   all_roc_data_sens[[model_name]] <<- data.frame(
#     FPR   = 1 - roc_obj$specificities,
#     TPR   = roc_obj$sensitivities,
#     Model = model_name
#   )
# 
#   # store performance metrics
#   all_model_performance_metrics_sens[[model_name]] <<- list(
#     Accuracy        = cm$overall["Accuracy"],
#     Sensitivity     = cm$byClass["Sensitivity"],
#     Specificity     = cm$byClass["Specificity"],
#     Precision       = cm$byClass["Precision"],
#     F1_Score        = cm$byClass["F1"],
#     AUC             = as.numeric(auc(roc_obj)),
#     Best_Threshold  = optimal_threshold_sens
#   )
# 
#   # store params
#   if (!is.null(trained_model$bestTune)) {
#     param_str <- paste(names(trained_model$bestTune),
#                        trained_model$bestTune,
#                        sep = "=", collapse = ", ")
#   } else param_str <- "—"
#   all_model_parameters_sens[[model_name]] <<- param_str
# 
#   cat(paste0("Evaluation for ", model_name, " (sens-focus) completed.\n"))
#   return(trained_model)
# }
# 
# 
# # -------------------------------------------------------------------
# # Function: Create comparison table for sensitivity-focused models
# # -------------------------------------------------------------------
# create_comparison_table_sens <- function() {
#   if (length(all_model_performance_metrics_sens) == 0) {
#     cat("No sensitivity-focused metrics available.\n")
#     return(NULL)
#   }
# 
#   df_summary_sens <- lapply(names(all_model_performance_metrics_sens), function(m) {
#     mtrs  <- all_model_performance_metrics_sens[[m]]
#     params<- all_model_parameters_sens[[m]]
#     data.frame(
#       Model           = m,
#       Accuracy        = round(mtrs$Accuracy, 4),
#       F1_Score        = round(mtrs$F1_Score, 4),
#       AUC             = round(mtrs$AUC, 4),
#       Sensitivity     = round(mtrs$Sensitivity, 4),
#       Specificity     = round(mtrs$Specificity, 4),
#       Precision       = round(mtrs$Precision, 4),
#       Best_Threshold  = round(mtrs$Best_Threshold, 4),
#       Parameters      = params,
#       stringsAsFactors = FALSE
#     )
#   }) %>% bind_rows()
# 
#   return(df_summary_sens)
# }
```

## 4.2: Logistic Regression

```{r}
# # --- Section 4.4.1: Logistic Regression (Optimised) ---
# 
# library(caret)
# library(dplyr)
# 
# cat("\n--- Optimised Logistic Regression (NZV + Corr Filtering) ---\n")
# 
# # Set seed for reproducibility
# set.seed(123)
# 
# # 1) Start from SMOTE-balanced training data
# train_lr_opt <- train_smote_universal
# 
# # 1.1 Remove near-zero variance predictors
# nzv_idx <- nearZeroVar(train_lr_opt)
# if (length(nzv_idx) > 0) {
#   train_lr_opt <- train_lr_opt[ , -nzv_idx]
#   cat("Removed", length(nzv_idx), "NZV predictors.\n")
# } else {
#   cat("No NZV predictors found.\n")
# }
# 
# # 1.2 Remove highly correlated numeric predictors (|r| > 0.9)
# numeric_vars <- train_lr_opt %>% select(where(is.numeric)) %>% names()
# if (length(numeric_vars) > 1) {
#   corr_m <- cor(train_lr_opt[ , numeric_vars])
#   high_corr <- findCorrelation(corr_m, cutoff = 0.9)
#   if (length(high_corr) > 0) {
#     removed_vars <- numeric_vars[high_corr]
#     train_lr_opt <- train_lr_opt %>% select(-all_of(removed_vars))
#     cat("Removed", length(removed_vars), "highly correlated predictors: ",
#         paste(removed_vars, collapse = ", "), "\n")
#   } else {
#     cat("No highly correlated predictors found.\n")
#   }
# } else {
#   cat("Not enough numeric predictors to check correlation.\n")
# }
# 
# # 2) Align test data
# final_preds <- setdiff(names(train_lr_opt), "MISKIN_KAKO")
# test_lr_opt <- test_data_universal %>%
#   select(all_of(final_preds)) %>%
#   mutate(MISKIN_KAKO = test_data_universal$MISKIN_KAKO)
# 
# # 3) Train and evaluate
# model_lr_opt <- train_and_evaluate_model(
#   train_data           = train_lr_opt,
#   test_data            = test_lr_opt,
#   target_variable_name = "MISKIN_KAKO",
#   model_name           = "Logistic Regression (opt)",
#   method_caret         = "glm",
#   model_formula        = MISKIN_KAKO ~ .
# )
# 
# cat("\nOptimised Logistic Regression complete.\n")
# 
# # 4) Summary output only (plots and table saved globally)
# df_summary <- create_comparison_table()
# print(df_summary)
# 
# plot_cv_accuracy_boxplot()
# plot_all_roc_curves()
```

```{r}
# # Pastikan package yang dibutuhkan sudah terinstal dan dimuat
# # install.packages(c("caret", "ggplot2", "dplyr", "tibble"))
# 
# library(caret)
# library(ggplot2)
# library(dplyr)
# library(tibble)
# 
# # --- Anggap 'model_lr_opt' adalah model Anda yang sudah dilatih ---
# # model_lr_opt <- # ... hasil dari fungsi train_and_evaluate_model Anda
# 
# # Langkah 1: Ekstrak feature importance dari model
# importance_object <- varImp(model_lr_opt, scale = FALSE)
# 
# # Langkah 2: Siapkan data untuk plotting
# # Ubah menjadi data frame, tambahkan nama fitur sebagai kolom, dan ambil 20 teratas
# importance_df <- importance_object$importance %>%
#   as.data.frame() %>%
#   rownames_to_column(var = "Fitur") %>%
#   rename(Importance = Overall) %>%
#   arrange(desc(Importance)) %>%
#   top_n(20) # Ambil 20 fitur paling berpengaruh
# 
# cat("Top 20 Fitur Paling Berpengaruh:\n")
# print(importance_df)
# 
# 
# # Langkah 3: Buat bar chart menggunakan ggplot2
# ggplot(importance_df, aes(x = Importance, y = reorder(Fitur, Importance))) +
#   geom_bar(stat = "identity", fill = "steelblue") +
#   labs(
#     title = "Top 20 Fitur Paling Berpengaruh terhadap MISKIN_KAKO",
#     subtitle = "Berdasarkan Model Regresi Logistik",
#     x = "Tingkat Kepentingan (Importance)",
#     y = "Nama Fitur"
#   ) +
#   theme_minimal() +
#   theme(
#     plot.title = element_text(face = "bold"),
#     axis.text.y = element_text(size = 9)
#   )
```

### Variable Importance Summary

```{r}
# # --- Section 4.5.2: Distribution of Top 4 Predictors by MISKIN_KAKO ---
# 
# library(dplyr)
# library(ggplot2)
# library(scales)
# 
# # 1) Prepare the data for plotting
# plot_df <- train_lr_opt %>%
#   mutate(
#     # ensure the target is a factor
#     MISKIN_KAKO = factor(MISKIN_KAKO, levels = c(0, 1),
#                          labels = c("NotPoor", "Poor")),
#     # convert the two dummy vars to factors with readable labels
#     R2001H_No = factor(R2001H_factor.No, levels = c(0, 1),
#                        labels = c("Yes", "No")),
#     R1808_Ceramic = factor(R1808_factor.Ceramic.tiles, levels = c(0, 1),
#                            labels = c("Other", "Ceramic Tiles"))
#   )
# 
# # 2) Boxplot: Number of children by poverty status
# ggplot(plot_df, aes(x = MISKIN_KAKO, y = n_children, fill = MISKIN_KAKO)) +
#   geom_boxplot(outlier.shape = 21, outlier.fill = "white") +
#   labs(
#     title = "Distribution of Number of Children by Poverty Status",
#     x = "Poverty Status",
#     y = "Number of Children"
#   ) +
#   theme_minimal() +
#   theme(legend.position = "none")
# 
# # 3) Boxplot: Proportion of current smokers by poverty status
# ggplot(plot_df, aes(x = MISKIN_KAKO, y = prop_current_smokers, fill = MISKIN_KAKO)) +
#   geom_boxplot(outlier.shape = 21, outlier.fill = "white") +
#   labs(
#     title = "Distribution of Proportion of Current Smokers by Poverty Status",
#     x = "Poverty Status",
#     y = "Proportion of Current Smokers"
#   ) +
#   theme_minimal() +
#   theme(legend.position = "none")
# 
# # 4) Proportion bar plot: R2001H (No) by poverty status
# ggplot(plot_df, aes(x = R2001H_No, fill = MISKIN_KAKO)) +
#   geom_bar(position = "fill") +
#   scale_y_continuous(labels = percent_format()) +
#   labs(
#     title = "Proportion of Households with R2001H='No', by Poverty Status",
#     x = "R2001H = No",
#     y = "Proportion"
#   ) +
#   theme_minimal()
# 
# # 5) Proportion bar plot: R1808 (Ceramic tiles) by poverty status
# ggplot(plot_df, aes(x = R1808_Ceramic, fill = MISKIN_KAKO)) +
#   geom_bar(position = "fill") +
#   scale_y_continuous(labels = percent_format()) +
#   labs(
#     title = "Proportion of Households with Ceramic Tile Flooring, by Poverty Status",
#     x = "Flooring = Ceramic Tiles",
#     y = "Proportion"
#   ) +
#   theme_minimal()
```

```{r}
# str(train_lr_opt$R2001H_factor.No)
# # unique(train_lr_opt$R2001H_factor.No)
# 
# str(train_lr_opt$R1808_factor.Ceramic.tiles)
# # unique(train_lr_opt$R1808_factor.Ceramic.tiles)
```

```{r}
# sum(train_lr_opt$R2001H_factor.No  %in% c(0,1))
# sum(train_lr_opt$R1808_factor.Ceramic.tiles %in% c(0,1))
```

## 4.3 Decision Tree

```{r}
# --- Section 4.4.2: Decision Tree Model (Final Version) ---

# 1. Load necessary libraries
# Make sure these are loaded at the start of your session.
library(caret)
library(dplyr)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(tibble)

cat("\n--- Training Decision Tree Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_dt <- train_smote_universal

nzv_cols <- nearZeroVar(train_dt, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_dt <- train_dt[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_dt_predictors <- setdiff(names(train_dt), "MISKIN_KAKO")
test_dt <- test_data_universal %>%
  select(all_of(final_dt_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_dt), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_dt), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# Specify the complexity parameter ('cp') values to test during cross-validation.
dt_tune_grid <- expand.grid(cp = c(0.001, 0.005, 0.01, 0.02, 0.05))

# 5. Train and Evaluate the Model
# Call our reusable function to train the model and get performance metrics.
set.seed(789) # for reproducibility
model_dt <- train_and_evaluate_model(
  train_data           = train_dt,
  test_data            = test_dt,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "Decision Tree",
  method_caret         = "rpart",
  tuneGrid             = dt_tune_grid
)

cat("\nDecision Tree model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates plots specific to the decision tree model.
plot_dt_visuals <- function(model, top_n = 20) {
  
  # -- Variable Importance Plot --
  # Extract, format, and plot the top N most important variables.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#2ca02c", width = 0.8) +
    labs(
      title = paste("Top", top_n, "Variables - Decision Tree"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)

  # -- Decision Tree Diagram --
  # Plot the final, pruned tree structure.
  cat("\n--- Plotting the final pruned decision tree ---\n")
  rpart.plot(
    model$finalModel,
    type = 4,
    extra = 101,
    fallen.leaves = TRUE,
    cex = 0.7,
    main = "Final Pruned Decision Tree"
  )
}

# 7. Generate All Visuals and Summaries
# Create the specific plots for the Decision Tree.
plot_dt_visuals(model_dt)

# Update the overall comparison table and plots with the new model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

### 4.3.2 using sensitivity

```{r}
# # --- Section 4.4.3: Decision Tree (Sensitivity-Focus, min_spec=0.4) ---
# 
# # 1. Load necessary libraries (if not already loaded)
# library(caret)
# library(dplyr)
# library(rpart)
# library(rpart.plot)
# library(ggplot2)
# library(tibble)
# 
# cat("\n--- Sensitivity-Focused Decision Tree Model (min specificity = 0.4) ---\n")
# 
# # 2. Prepare Training Data (SMOTE + NZV filter)
# train_dt_sens <- train_smote_universal
# 
# nzv_idx <- nearZeroVar(train_dt_sens)
# if (length(nzv_idx) > 0) {
#   train_dt_sens <- train_dt_sens[, -nzv_idx]
#   cat("Removed", length(nzv_idx), "NZV predictors.\n")
# } else {
#   cat("No NZV predictors found.\n")
# }
# 
# # 3. Align Test Data
# final_dt_predictors <- setdiff(names(train_dt_sens), "MISKIN_KAKO")
# test_dt_sens <- test_data_universal %>%
#   select(all_of(final_dt_predictors), MISKIN_KAKO)
# 
# cat("Training dims:", paste(dim(train_dt_sens), collapse = " x "), "\n")
# cat("Test dims:    ", paste(dim(test_dt_sens), collapse = " x "), "\n")
# 
# # 4. Define Tuning Grid (same as F1 version)
# dt_tune_grid <- expand.grid(cp = c(0.001, 0.005, 0.01, 0.02, 0.05))
# 
# # 5. Train & Evaluate with sensitivity focus (min_spec = 0.4)
# set.seed(789)
# model_dt_sens <- train_and_evaluate_model_sens(
#   train_data           = train_dt_sens,
#   test_data            = test_dt_sens,
#   target_variable_name = "MISKIN_KAKO",
#   model_name           = "Decision Tree (sens-focus)",
#   method_caret         = "rpart",
#   tuneGrid             = dt_tune_grid,
#   min_specificity      = 0.4
# )
# 
# cat("\nSensitivity-focused Decision Tree complete.\n")
# 
# # 6. Model-Specific Visualisation (reuse existing function)
# plot_dt_visuals(model_dt_sens, top_n = 20)
# 
# # 7. Compare F1- vs Sensitivity-Optimized Results
# df_summary_f1   <- create_comparison_table()        # F1-based
# df_summary_sens <- create_comparison_table_sens()   # Sensitivity-based
# 
# cat("\n--- Comparison: F1 vs Sensitivity Thresholds ---\n")
# print(df_summary_f1)
# print(df_summary_sens)
# 
# # 8. (Optional) Re-plot diagnostics
# plot_cv_accuracy_boxplot()   # shows F1-models’ CV accuracy
# plot_all_roc_curves()        # overlays F1-models’ ROC curves
# # If you’d like to overlay sens-focus curves, you can write a similar ggplot using all_roc_data_sens
```

## 4.4 Random Forest

```{r}
# --- Section 4.4.3: Random Forest Model (Final Version) ---

# 1. Load necessary libraries
library(caret)
library(dplyr)
library(ranger) # The 'ranger' engine for Random Forest

cat("\n--- Training Random Forest Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_rf <- train_smote_universal

nzv_cols <- nearZeroVar(train_rf, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_rf <- train_rf[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_rf_predictors <- setdiff(names(train_rf), "MISKIN_KAKO")
test_rf <- test_data_universal %>%
  select(all_of(final_rf_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_rf), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_rf), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# Dynamically create a grid to test 'mtry' and 'min.node.size'.
num_predictors <- length(final_rf_predictors)
mtry_default <- floor(sqrt(num_predictors))
mtry_values <- unique(pmax(1, c(mtry_default - 2, mtry_default, mtry_default + 2)))

rf_tune_grid <- expand.grid(
  mtry = mtry_values,
  min.node.size = c(1, 5, 10),
  splitrule = "gini"
)

cat("Tuning grid created for Random Forest.\n")

# 5. Train and Evaluate the Model
# Call our reusable function, passing 'ranger'-specific arguments.
set.seed(2025) # for reproducibility
model_rf <- train_and_evaluate_model(
  train_data           = train_rf,
  test_data            = test_rf,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "Random Forest",
  method_caret         = "ranger",
  tuneGrid             = rf_tune_grid,
  importance           = 'impurity' # Specific argument for ranger
)

cat("\nRandom Forest model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates the variable importance plot for the RF model.
plot_rf_visuals <- function(model, top_n = 20) {
  
  # Extract, format, and plot the top N most important variables.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#1f77b4", width = 0.8) + # Blue fill color
    labs(
      title = paste("Top", top_n, "Variables - Random Forest"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)
}

# 7. Generate All Visuals and Summaries
# Create the specific plots for the Random Forest.
plot_rf_visuals(model_rf)

# Update the overall comparison table and plots with the new model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

## 4.5 XGBoost

```{r}
# --- Section 4.4.4: XGBoost Model (Final Version) ---

# 1. Load necessary libraries
# Make sure these are loaded at the start of your session.
library(caret)
library(dplyr)
library(xgboost) # The 'xgboost' engine
library(ggplot2)

cat("\n--- Training XGBoost Model ---\n")

# 2. Prepare Training Data
# Start with the SMOTE-balanced data and apply the NZV filter.
train_xgb <- train_smote_universal

nzv_cols <- nearZeroVar(train_xgb, saveMetrics = FALSE)
if (length(nzv_cols) > 0) {
  train_xgb <- train_xgb[, -nzv_cols]
  cat(paste("Removed", length(nzv_cols), "near-zero variance predictors.\n"))
} else {
  cat("No NZV predictors found.\n")
}

# 3. Align Test Data
# Ensure the test set has the exact same columns as the processed training set.
final_xgb_predictors <- setdiff(names(train_xgb), "MISKIN_KAKO")
test_xgb <- test_data_universal %>%
  select(all_of(final_xgb_predictors), MISKIN_KAKO)

cat(paste("Training data dimensions:", paste(dim(train_xgb), collapse = "x"), "\n"))
cat(paste("Test data dimensions:    ", paste(dim(test_xgb), collapse = "x"), "\n"))

# 4. Define Tuning Grid
# As you specified, we will tune nrounds, max_depth, and eta.
xgb_tune_grid <- expand.grid(
  nrounds = c(100, 200),
  max_depth = c(3, 6),
  eta = c(0.1, 0.3),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

cat("Tuning grid created for XGBoost.\n")

# 5. Train and Evaluate the Model
# Call our reusable function to train the model and get performance metrics.
set.seed(303) # for reproducibility
model_xgb <- train_and_evaluate_model(
  train_data           = train_xgb,
  test_data            = test_xgb,
  target_variable_name = "MISKIN_KAKO",
  model_name           = "XGBoost",
  method_caret         = "xgbTree",
  tuneGrid             = xgb_tune_grid,
  verbose              = FALSE # Suppress XGBoost's verbose output
)

cat("\nXGBoost model training and evaluation complete.\n")

# 6. Model-Specific Visualisation Function
# This function creates the variable importance plot for the XGBoost model.
plot_xgb_visuals <- function(model, top_n = 20) {
  
  # Extract, format, and plot the top N most important variables using caret's method.
  importance_df <- varImp(model, scale = TRUE)$importance %>%
    as.data.frame() %>%
    tibble::rownames_to_column("Variable") %>%
    rename(Importance = Overall) %>%
    arrange(desc(Importance)) %>%
    slice_head(n = top_n)

  importance_plot <- ggplot(importance_df, aes(x = Importance, y = reorder(Variable, Importance))) +
    geom_col(fill = "#d62728", width = 0.8) + # Red fill color
    labs(
      title = paste("Top", top_n, "Variables - XGBoost"),
      x = "Importance Score",
      y = "Variable"
    ) +
    theme_minimal()
  
  print(importance_plot)
}

# 7. Generate All Visuals and Summaries
# Create the specific plots for XGBoost.
plot_xgb_visuals(model_xgb)

# Update the overall comparison table and plots with the final model's results.
df_summary <- create_comparison_table()
plot_cv_accuracy_boxplot()
plot_all_roc_curves()
```

```{r}
# --- Section 4.5.3: XGBoost – SHAP & 2-way PDP (Corrected) ---

library(SHAPforxgboost)
library(pdp)
library(ggplot2)
library(dplyr)

# 1) Prepare X_test matrix (no change)
X_test <- test_xgb %>%
  select(-MISKIN_KAKO) %>%
  as.matrix()

# 2) Extract the native xgboost model
xgb_native <- model_xgb$finalModel

# 3) Compute SHAP values
shap_vals   <- shap.values(xgb_model = xgb_native, X_train = X_test)
shap_score  <- shap_vals$shap_score      # SHAP contributions
mean_shap   <- shap_vals$mean_shap_score # avg(|SHAP|) per feature

# 4) Prepare for plotting
shap_long <- shap.prep(
  shap_contrib = shap_score,
  X_train      = X_test
)

# 5) Global SHAP summary plot
shap.plot.summary(shap_long) + 
  ggtitle("SHAP Summary: Feature Importance & Effect Direction")

# 6) SHAP dependence plots for top 5 features
top5 <- names(sort(mean_shap, decreasing = TRUE))[1:5]
for(feat in top5) {
  p <- shap.plot.dependence(
    data_long     = shap_long,
    x             = feat,
    color_feature = feat,
    alpha         = 0.5
  ) +
    ggtitle(paste("SHAP Dependence Plot for", feat))
  print(p)
}

# 7) Two‐way Partial Dependence (top 2 features)
top2 <- top5[1:2]

# Define pred.fun that accepts a data.frame
pred_fun <- function(object, newdata) {
  predict(object, as.matrix(newdata))
}

pdp_inter <- partial(
  object         = xgb_native,
  pred.var       = top2,
  train          = as.data.frame(X_test),
  pred.fun       = pred_fun,
  grid.resolution= 25,
  prob           = TRUE,
  progress       = "none"
)

# 8) Plot 2‐way PDP as contour
autoplot(pdp_inter, contour = TRUE) +
  labs(
    title = paste("2-way Partial Dependence:", top2[1], "vs", top2[2]),
    x     = top2[1],
    y     = top2[2],
    fill  = "Predicted\nPoverty\nProb."
  ) +
  theme_minimal()
```

```{r}
# --- Section 4.5.4: Top-15 SHAP Summary & Table ---

library(dplyr)
library(ggplot2)

# 1) Compute signed mean SHAP from the raw SHAP‐matrix:
#    shap_score is the matrix from shap.values()
mean_signed_shap <- colMeans(shap_score)    

# 2) Build a data frame of importance & direction, then pick top 15
shap_summary15 <- data.frame(
  Feature        = names(mean_shap),
  Mean_Abs_SHAP  = mean_shap,
  Mean_SHAP      = mean_signed_shap[names(mean_shap)],
  stringsAsFactors = FALSE
) %>%
  arrange(desc(Mean_Abs_SHAP)) %>%
  slice_head(n = 15)

# 3) Print the table
cat("\nTop 15 Features by Mean(|SHAP|) — Importance & Effect Direction:\n")
print(shap_summary15)

# 4) Filter the long SHAP frame to just those features
shap_long15 <- shap_long %>%
  filter(variable %in% shap_summary15$Feature)

# 5) SHAP summary plot for only the top 15
shap.plot.summary(shap_long15) +
  ggtitle("SHAP Summary Plot — Top 15 Features") +
  theme_minimal()
```

```{r}
# 1) Filter to top 15 and set factor levels so only these appear, in descending importance
shap_long15 <- shap_long %>%
  filter(variable %in% shap_summary15$Feature) %>%
  mutate(
    variable = factor(variable,
                      levels = shap_summary15$Feature)  # preserves the order from shap_summary15
  )

# 2) Plot summary for just those 15
shap.plot.summary(shap_long15) +
  ggtitle("SHAP Summary Plot — Top 15 Features Only") +
  theme_minimal()

shap.plot.summary(shap_long15) +
  ggtitle("SHAP Summary Plot — Top 15 Features Only") +
  theme_minimal() +
  theme(
    legend.position  = "bottom",
    legend.direction = "horizontal"
  )
```

### shap no R301

```{r}
# --- Section 4.5.3: XGBoost – SHAP & 2-way PDP (R301 Removed) ---

library(SHAPforxgboost)
library(pdp)
library(ggplot2)
library(dplyr)

# 1) Prepare X_test matrix
X_test <- test_xgb %>%
  select(-MISKIN_KAKO) %>%
  as.matrix()

# 2) Extract the native xgboost model
xgb_native <- model_xgb$finalModel

# 3) Compute SHAP values
shap_vals  <- shap.values(xgb_model = xgb_native, X_train = X_test)
shap_score <- shap_vals$shap_score      # full SHAP contributions matrix
mean_shap  <- shap_vals$mean_shap_score # full |mean SHAP| vector

# 4) Prepare long SHAP table
shap_long <- shap.prep(
  shap_contrib = shap_score,
  X_train      = X_test
)

# 5) Remove R301 from both mean_shap and shap_long
mean_shap_noR301   <- mean_shap[names(mean_shap) != "R301"]
shap_long_noR301   <- shap_long %>% 
  filter(variable != "R301")

# 6) Global SHAP summary plot (no R301)
shap.plot.summary(shap_long_noR301) +
  ggtitle("SHAP Summary: Top Features (R301 Excluded)") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.direction = "horizontal")

# 7) SHAP dependence plots for top 5 (excluding R301)
top5_noR301 <- names(sort(mean_shap_noR301, decreasing = TRUE))[1:5]
for (feat in top5_noR301) {
  p <- shap.plot.dependence(
    data_long     = shap_long_noR301,
    x             = feat,
    color_feature = feat,
    alpha         = 0.5
  ) +
    ggtitle(paste("SHAP Dependence Plot for", feat))
  print(p)
}

# 8) Two‐way Partial Dependence on the top 2 of those
top2_noR301 <- top5_noR301[1:2]

pred_fun <- function(object, newdata) {
  predict(object, as.matrix(newdata))
}

pdp_inter_noR301 <- partial(
  object         = xgb_native,
  pred.var       = top2_noR301,
  train          = as.data.frame(X_test),
  pred.fun       = pred_fun,
  grid.resolution= 25,
  prob           = TRUE,
  progress       = "none"
)

# 9) Plot 2-way PDP contour
autoplot(pdp_inter_noR301, contour = TRUE) +
  labs(
    title = paste("2-way Partial Dependence:", top2_noR301[1], "vs", top2_noR301[2]),
    x     = top2_noR301[1],
    y     = top2_noR301[2],
    fill  = "Predicted\nPoverty\nProb."
  ) +
  theme_minimal()
```

```{r}
# --- Section 4.5.5: SHAP Summary — Top 15 (without R301) ---

library(dplyr)
library(ggplot2)

# 1) Rebuild the SHAP‐summary table, excluding R301
mean_signed_shap <- colMeans(shap_score)

shap_summary15_noR301 <- data.frame(
  Feature       = names(mean_shap),
  Mean_Abs_SHAP = mean_shap,
  Mean_SHAP     = mean_signed_shap[names(mean_shap)],
  stringsAsFactors = FALSE
) %>%
  filter(Feature != "R301") %>%          # drop R301
  arrange(desc(Mean_Abs_SHAP)) %>%
  slice_head(n = 15)                     # keep top 15

# 2) Filter the long SHAP table to those 15 features
shap_long15_noR301 <- shap_long %>%
  filter(variable %in% shap_summary15_noR301$Feature) %>%
  mutate(
    variable = factor(
      variable,
      levels = shap_summary15_noR301$Feature  # ensures correct y‐order
    )
  )

# 3) Print the Top-15 table for reference
cat("\nTop 15 SHAP Features (excluding R301):\n")
print(shap_summary15_noR301)

# 4) Plot the SHAP summary for just those 15
shap.plot.summary(shap_long15_noR301) +
  ggtitle("SHAP Summary Plot — Top 15 Features (R301 Removed)") +
  theme_minimal() +
  theme(
    legend.position  = "bottom",
    legend.direction = "horizontal"
  )
```

```{r}
# --- Section 4.5.6: Export XGBoost Feature Importance & Tree Plot ---

library(xgboost)
library(Ckmeans.1d.dp)   # xgb.plot.importance dependency
library(ggplot2)

# 1) Compute feature importance matrix
#    Use the same predictor names you trained on
feature_names <- colnames(test_xgb %>% select(-MISKIN_KAKO))
importance_matrix <- xgb.importance(
  feature_names = feature_names,
  model         = model_xgb$finalModel
)

# 2) Print the top 20 features
print(importance_matrix[1:20, ])

# 3) Plot & save the Importance chart
#    (uses Ckmeans.1d.dp to cluster for coloring)
png("xgb_feature_importance.png", width = 800, height = 600, res = 120)
xgb.plot.importance(
  importance_matrix = importance_matrix,
  top_n             = 20,
  measure           = "Gain",        # you can also choose Cover or Frequency
  rel_to_first      = TRUE,
  xlab              = "Relative Importance (Gain)"
)
dev.off()

# 4) Export a single tree (e.g. the first tree) to PDF
pdf("xgb_tree_0.pdf", width = 10, height = 8)
xgb.plot.tree(
  model = model_xgb$finalModel,
  trees = 0,                # zero‐indexed: the very first tree
  show_node_id = TRUE
)
dev.off()

# 5) (Optional) If you want a ggplot version of importance:
library(dplyr)
imp_df <- importance_matrix %>%
  slice_max(order_by = Gain, n = 20) %>%
  mutate(Feature = factor(Feature, levels = rev(Feature)))

ggplot(imp_df, aes(x = Gain, y = Feature)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "XGBoost Top 20 Feature Importance by Gain",
    x     = "Gain (relative)",
    y     = "Feature"
  ) +
  theme_minimal()

# You can save that ggplot with:
# ggsave("xgb_feature_importance_ggplot.png", width=8, height=6)
```

# Section 5: Export Comparison

```{r}
# # create an output directory (optional)
# output_dir <- "rds_outputs"
# if (!dir.exists(output_dir)) dir.create(output_dir)
# 
# # 1. Comparison table
# comparison_tbl <- create_comparison_table()
# saveRDS(
#   comparison_tbl,
#   file = file.path(output_dir, "comparison_table_HH_Ignorance.rds")
# )
# 
# # 2. CV‐accuracy boxplot data
# cv_accuracy_df <- bind_rows(all_cv_accuracy_results) %>% 
#   filter(is.finite(Accuracy))
# saveRDS(
#   cv_accuracy_df,
#   file = file.path(output_dir, "cv_accuracy_data_HH_Ignorance.rds")
# )
# 
# # 3. ROC curve data
# roc_data_df <- bind_rows(all_roc_data)
# saveRDS(
#   roc_data_df,
#   file = file.path(output_dir, "roc_curve_data_HH_Ignorance.rds")
# )
# 
# cat("✔️  Three RDS files written to", output_dir, "\n",
#     "- comparison_table_HH_Ignorance.rds\n",
#     "- cv_accuracy_data_HH_Ignorance.rds\n",
#     "- roc_curve_data_HH_Ignorance.rds\n")
```
